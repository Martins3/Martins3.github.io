# 3 Concurrent Objects
*Quiescent consistency* is appropriate for applications that require high performance at the cost of
placing relatively weak constraints on object behavior. *Sequential consistency* is
a stronger condition, often useful for describing low-level systems such as hardware memory interfaces. *Linearizability*, even stronger, is useful for describing
higher-level systems composed from linearizable components.

Along a different dimension, different method implementations provide different progress guarantees. Some are *blocking*, where the delay of any one thread
can delay others, and some are *nonblocking*, where the delay of a thread cannot
delay the others

#### 3.1 Concurrency and Correctness
What does it mean for a concurrent object to be correct ?

We therefore need a way to specify the behavior of concurrent objects,
and to reason about their implementations, without relying on method-level locking.

Nevertheless, the lock-based queue example illustrates a useful principle: *it
is easier to reason about concurrent objects if we can somehow map their concurrent executions to sequential ones, and limit our reasoning to these sequential
executions.*
> 将并行执行映射为顺序执行的一般方法是什么

#### 3.2 Sequential Objects
The API documentation typically says something like the following: if the
object is in such-and-such a state before you call the method, then the object
will be in some other state when the method returns, and the call will return a
particular value, or throw a particular exception.

This kind of description divides
naturally into a **precondition** (describing the object’s state before invoking the
method) and a **postcondition**, describing, once the method returns, the object’s
state and return value.


A change to an object’s state is sometimes called a **side effect**.

This style of documentation, called a **sequential specification**, is so familiar that it is easy to overlook how elegant and powerful it is.
> @todo_linear 这和 linearizable specification 是类似的东西吗 ?

The object’s documentation describes the object
state before and after each call, and we can safely ignore any intermediate states
that the object may assume while the method call is in progress.

Defining objects in terms of preconditions and postconditions makes perfect
sense in a sequential model of computation where a single thread manipulates a
collection of objects


#### 3.3 Quiescent Consistency
For historical reasons, the object version of a read–write memory location is
called a **register**.

Principle 3.3.1. Method calls should appear to happen in a one-at-a-time, sequential order.

An object is **quiescent** if it has no pending method calls.

Principle 3.3.2. Method calls separated by a period of quiescence should appear to take effect in their real-time order.

Together, Principles 3.3.1 and 3.3.2 define a correctness property called **quiescent consistency**.

Informally, it says that any time an object becomes quiescent, then the execution so far is equivalent to some sequential execution of the completed calls.

##### 3.3.1 Remark
How much does quiescent consistency limit concurrency? Specifically, under
what circumstances does quiescent consistency require one method call to block
waiting for another to complete? Surprisingly, the answer is (essentially), never


*In any concurrent execution, for any pending invocation of a total method,
there exists a quiescently consistent response. This observation does not mean
that it is easy (or even always possible) to figure out what that response is, but only
that the correctness condition itself does not stand in the way. We say that quiescent consistency is a nonblocking correctness condition. We make this notion
more clear in Section 3.6*

A correctness property P is **compositional** if, whenever each object in the system satisfies P, the system as a whole satisfies P.

quiescent consistency is compositional, so quiescently consistent objects can be composed to construct more complex quiescently consistent objects.

#### 3.4 Sequential Consistent
The order in which a single thread issues method calls is called its program order. (Method calls by different threads are unrelated by program order.)

Principle 3.4.1. Method calls should appear to take effect in program order

Together, **Principles 3.3.1** and **3.4.1** define a correctness property called
sequential consistency, which is widely used in the literature on multiprocessor
synchronization.
> 函数调用没有重叠(@todo )，而且按照程序顺序
> 其实不能理解 **Principles 3.3.1** 的含义是什么 ?


##### 3.4.1 Remarks
It is worth noting that sequential consistency and quiescent consistency are
incomparable: there exist sequentially consistent executions that are not quiescently consistent, and vice versa

One could argue whether it is acceptable to reorder method calls whose intervals do not overlap, even if they occur in different threads.

Sequential consistency, like quiescent consistency, is nonblocking: any pending call to a total method can always be completed.

Is sequential consistency compositional? That is, is the result of composing multiple sequentially consistent objects itself sequentially consistent? Here,
unfortunately, the answer is **no**.

Sequential consistency requires that method calls act as if they occurred in a
sequential order consistent with program order. That is, in any concurrent execution, there is a way to order the method calls sequentially so that they (1) are
consistent with program order, and (2) meet the object’s *sequential specification*.

#### 3.5 Linearizability
We have seen that the principal drawback of sequential consistency is that it is not
compositional: the result of composing sequentially consistent components is not
itself necessarily sequentially consistent. We propose the following way out of this
dilemma. Let us replace the requirement that method calls appear to happen in
program order with the following stronger restriction:

Principle 3.5.1. Each method call should appear to take effect instantaneously at
some moment between its invocation and response.

This principle states that the *real-time behavior of method* calls must be preserved.
We call this correctness property linearizability. Every linearizable execution is sequentially consistent, but not vice versa.

##### 3.5.1 Linearization Point
How can we tell whether an object is really a FIFO queue? We simply assume
that we have some effective way of recognizing whether any sequential object
history is or is not a legal history for that object’s class.
A *sequential specification* for an object is just a set of sequential histories for the object. A sequential history `H` is legal if each object subhistory is legal for that object

The usual way to show that a concurrent object implementation is linearizable is
to identify for each method a linearization point where the method takes effect.
For lock-based implementations, each method’s critical section can serve as its
linearization point. For implementations that do not use locking, the linearization point is typically a single step where the effects of the method call become
visible to other method calls.

##### 3.5.2 Remarks
Sequential consistency is a good way to describe standalone systems, such as
hardware memories, where composition is not an issue. Linearizability, by contrast, is a good way to describe components of large systems, where components
must be implemented and verified independently. Moreover, the techniques we
use to implement concurrent objects, are all linearizable. 

How much does linearizability limit concurrency? Linearizability, like sequential consistency, is nonblocking. Moreover, like quiescent consistency, but unlike
sequential consistency, linearizability is compositional; the result of composing
linearizable objects is linearizable


#### 3.6 Formal Definitions
Informally, we know that a concurrent object is linearizable if each method call
appears to take effect instantaneously at some moment between that method’s
invocation and return events.
> 如果使用lock 实现，就很容易理解了。

> 其实，这一个定义包含了程序执行在同一个thread中间的时候，顺序不能颠倒，当程序出现在不同的thread 中间的时候，如果没有出现重叠，那么顺序需要保证，如果出现重叠，顺序随便。
> 相当于给出一个可以重新排序的适用范围，当重排成为一个single thread 执行的时候，如果还是满足specification 那么就可以。
> 部分执行，需要很大的限度的重排，才可以保证满足。
> 给定了很大范围的重排，排出一些不满足的情况。

This statement is probably enough for most informal reasoning, but a more precise formulation is needed to take care of some
tricky cases (such as method calls that have not returned), and for more rigorous
styles of argument.
> 其实formal 定义就是为了处理一个执行片段中间存在没有函数返回的情况而已。

> skip about one page
##### 3.6.1 Linearizability
> 只要可以重排为一个合法的S即可
equivalent 含义 : 在每一个线程上执行的相等的。

> 感觉invoke 和 response 两个概念就是用来让人更加费解的。

合法的S 是什么 : 
A sequential history H is legal if each object subhistory is legal for that object.
> 针对于每一个对象是合法的，合法确实是发发针对于每一个对象的，就每一个线程而言，没有办法观测到所有的数据。
> 具体而言，就是queue 的push pop 总是对应的.

> sequential consistency 能否保证其thread 的顺序 ? 可以的, 首先是 equivalent 的，equivalent 就保证了其 thread 的顺序。
> 所以定义的过程: 通过equivalent 来保证trhead 中间的顺序，而偏序关系保证所有的关系。

##### 3.6.2 Compositional Linearizabili
Linearizability is compositional:

Theorem 3.6.1. **H is linearizable if, and only if, for each object x, H|x is linearizable**
> 1. 证明 : 还是没有看懂
> 2. 函数是可线性化的含义是 ? 感觉现在一直在讨论都是调用过程的可线性化

##### 3.6.3 The Nonblocking Property
Linearizability is a nonblocking property: a pending invocation of a total methodis never required to wait for another pending invocation to complet

Theorem 3.6.2. Let `inv(m)` be an invocation of a total method. If `<x inv P>` is
a pending invocation in a linearizable history H, then there exists a response
`<x res P>` such that `H · <x res P>` is linearizable
> 当 linearizable history 存在一个pending invocation，那么将其 response 添加上，
> 那么依旧可以为 linearizable 的。
> 只是说可以存在一个 response，而且该 response 的出现不会违背 linearizable 的性质
> 但是没有保证必然会出现!

This theorem implies that linearizability by itself never *forces* a thread with a pending invocation of a total method to block.
> 现在你来给我找一个 force 别人block 的？
> 注意，当一个函数是由 corase lock 包围的，也是可线性化的

This theorem suggests that
linearizability is an appropriate correctness condition for systems where concurrency and real-time response are important.
> linearizability只是一个 appropriate correctness condition内容，而不是必须内容


wait free之类，表示的为blocking，处理到一个thread 执行会不会阻碍到其他的thread 运行的问题，linearizability 处理的是，一个thread 进行的修改其他的thread 如何观察到。


#### 3.7 Progress Conditions
Linearizability’s nonblocking property states that any pending invocation has a
correct response, but does not talk about how to compute such a response.

Such an implementation is called *blocking*, because an unexpected delay by one thread can prevent others from making progress.
> @todo_wait

A method is **wait-free** if it guarantees that every call finishes its execution
in a finite number of steps.

It is bounded wait-free if there is a bound on the number of steps a method call can take.

A wait-free method whose performance does not depend on the number of active
threads is called population-oblivious.

Being wait-free is an example
of a nonblocking progress condition, meaning that an arbitrary and unexpected
delay by one thread (say, the one holding a lock) does not necessarily prevent the
others from making progress.
> @todo_wait 相同观点反复说明，不要出现一个人阻塞其他人!

The wait-free property is attractive because it guarantees that every thread that
takes steps makes progress. However, wait-free algorithms can be inefficient, and
sometimes we are willing to settle for a weaker nonblocking property.
> @todo_wait wait free 从来没有保证过效率 !

A method is lock-free if it guarantees that *infinitely* often some method call
finishes in a finite number of steps. Clearly, any wait-free method implementation
is also lock-free, but not vice versa. Lock-free algorithms admit the possibility
that some threads could starve. 
> 常常是


##### 3.7.1 Dependent Progress Condition
**The wait-free and lock-free nonblocking progress conditions guarantee that the
computation as a whole makes progress, independently of how the system schedules threads.**

In Chapter 2 we encountered two progress conditions for blocking implementations: the deadlock-free and starvation-free properties. These properties
are dependent progress conditions: progress occurs only if the underlying platform (i.e., the operating system) provides certain guarantees. In principle, the
deadlock-free and starvation-free properties are useful when the operating system guarantees that every thread eventually leaves every critical section. In practice, these properties are useful when the operating system guarantees that every
thread eventually leaves every critical section in a timely manner.
> @todo 找到总结的表格，deadlock-free 和 starvation-free 的功能是什么?


There is also a *dependent nonblocking progress condition*: the obstruction-free
property. We say that a method call executes in isolation if no other threads take
steps.

Definition 3.7.1. A method is **obstruction-free** if, from any point after which it
executes in isolation, it finishes in a finite number of steps

Like the other nonblocking progress conditions, the obstruction-free condition ensures that not all threads can be blocked by a sudden delay of one or more
other threads. A lock-free algorithm is obstruction-free, but not vice versa.

The simplest way to exploit an obstructionfree algorithm is to introduce a back-off mechanism: a thread that detects a con-
flict pauses to give an earlier thread time to finish

#### 3.8 The Java Memory Mod
> skip


#### 3.9 Remark
*Which correctness condition is right for one’s application? Well, it depends
on the needs of the application. A lightly loaded printer server that uses a queue
to hold, say print jobs, might be satisfied with a quiescently-consistent queue,
since the order in which documents are printed is of little importance. A banking
server should execute customer requests in program order (transfer $100 from
savings to checking, write a check for $50), so it should use a sequentially consistent queue. A stock-trading server is required to be fair, so orders from different
customers must be executed in the order they arrive, so it would require a linearizable queue*
> 给出这些场景，也不知道如何套用啊!

#### 3.10 Chapter Notes

