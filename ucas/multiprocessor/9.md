# Linked Lists: The Role of Locking

## 9.1 Introduction
This chapterintroduces several useful techniques that go beyond coarse-grained
locking to allow multiple threads to access a single object at the same time.
- Fine-grained synchronization
- Optimistic synchronization : Many objects, such as trees or lists, consist of multiple components linked together by references. Some methods search for a
particular component (e.g., a list or tree node containing a particular key).
*One way to reduce the cost of fine-grained locking is to search without acquiring any locks at all*. If the method finds the sought-after component, it locks
that component, and then checks that the component has not changed in the
interval between when it was inspected and when it was locked. This technique
is worthwhile only if it succeeds more often than not, which is why we call it
optimistic
- Lazy synchronization : Sometimes it makes sense to *postpone* hard work. For
example, the task of removing a component from a data structure can be split
into two phases: the component is *logically* removed simply by setting a tag bit,
and later, the component can be *physically* removed by unlinking it from the
rest of the data structure.
- Nonblocking synchronization: Sometimes we can eliminate locks entirely,
relying on built-in atomic operations such as compareAndSet() for synchronization.
> 痛苦的根源，实现

## 9.2 List-Based Sets
> 差不多在介绍什么是linkedlist

## 9.3 Concurrent Reasoning
> 蛇皮，根本不知道在搞什么?

Reasoning about concurrent data structures may seem impossibly difficult, but it
is a skill that can be learned. Often, the key to understanding a concurrent data
structure is to understand its **invariants**: properties that always hold.
We can show that a property is invariant by showing that:
1. The property holds when the object is created, and
2. Once the property holds, then *no thread can take a step that makes the property false*.

Most interesting invariants hold trivially when the list is created,
so it makes sense to focus on how invariants, once established, *are preserve*.

Specifically, we can check that each invariant is preserved by each invocation
of insert(), remove(), and contains() methods. This approach works only if
we can assume that these methods are the only ones that modify nodes, a property sometimes called *freedom from interference*.
In the list algorithms considered here, nodes are internal to the list implementation,
*so freedom from interference is guaranteed because users of the list have no opportunity to modify its internal nodes*.

We require freedom from interference even for nodes that have been removed
from the list, since some of our algorithms permit a thread to unlink a node
while it is being traversed by others.

When reasoning about concurrent object implementations, it is important to
understand the distinction between an object’s *abstract value* (here, a set of items),
and its *concrete representation* (here, a list of nodes)

Not every list of nodes is a meaningful representation for a set. An algorithm’s
representation invariant characterizes which representations make sense as
abstract values.
> ?

The set algorithms in this chapter require the following invariants (some
require more, as explained later). First, sentinels are neither added nor removed.
Second, nodes are sorted by key, and keys are unique.
> 默认sort 的状态

Given a list satisfying the representation invariant, which set does it represent?
The meaning of such a list is given by an *abstraction map* carrying lists that satisfy
the representation invariant to sets. Here, the abstraction map is simple: an item is in the set if and only if it is reachable from head.

What safety and liveness properties do we need? Our safety property is linearizability.

Some use locks, and care is required to ensure they are deadlock- and starvation-free. Some
nonblocking list algorithms do not use locks at all, while others restrict locking
to certain methods. Here is a brief summary, from Chapter 3, of the nonblocking
properties we use :
- A method is wait-free if it guarantees that every call finishes in a finite number
of steps.
- A method is lock-free if it guarantees that some call always finishes in a finite
number of steps.

## 9.4 Coarse-Grained Synchronization
> 小学生实现


## 9.5 Fine-Grained Synchronizati
We can improve concurrency by locking individual nodes, rather than locking the list as a whole。

> 找到一下使用一个lock 的 counter example :
> 只能说明在使用一个lock 的某种情况会出现问题，但是实际上没有完整论证
> add 两个锁，remove 一个锁还是不行!
>
> 感觉 add 一个lock(锁定pre) 而remove 两个lock 一样可以满足要求, 应该是没有问题的 !
>
> 所以对于连续add remove 都是lock pred curr 两个一定不会出现问题

## 9.6 Optimistic Synchronization
One way to reduce synchronization costs is to take a chance: search without
acquiring locks, lock the nodes found, and then confirm that the locked nodes
are correct. If a synchronization conflict causes the wrong nodes to be locked,
then release the locks and start over.
Normally, this kind of conflict is rare, which is why we call this technique optimistic synchronization.

> 上锁，不会阻止之后的人继续向前，但是fine-grained 中间会。
> 查询，对于两者上锁，检查是否合法，操作(因为此时已经上了两个锁)

> 问题是，上完锁之后，然后仍然需要检查，如果不检查:
> 关键的问题在于: 两个上锁不是原子的，导致，第一个锁上去之后，然后该节点就被删除了。(这种情况应该是不存在的)
> 第一个锁都没有上去之前，然后其他线程就将节点删除了，上锁已经晚了
> 上锁必须相同的方向上锁，否则死锁 !

> 其实需要处理的问题在于，上锁的得到的内容，其实已经删除了，所以首先需要遍历一遍查询当前的节点是否还在链表中间的

## 9.7 Lazy Synchronization
mark 的操作导致 validate 操作添加一个判断，同时去掉了循环，之前的循环的判断在问，lock到的节点是不是已经被去除了，
所以最简洁的方法，删除之前标记一下。

可能有人询问的问题，为什么直接直接 logical 删除，其实这就是关键 : 既然可以只是标记一下就完成的

## 9.8 Non-Blocking Synchronization
存在一个基本的小技巧，那就是上一条指令读到，下一条指令写，那么需要检查一下，读到的东西是否发生了改变，如果改变了，失败，继续循环，
否则，compareAndSet 就可以设置新的数值了。
