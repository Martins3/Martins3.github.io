# KVM

## TODO
1. çœ‹çœ‹ kvm çš„ ioctl çš„å®ç°
2. æ±‚æ±‚äº†ï¼Œä»€ä¹ˆæ—¶å€™å­¦ä¸€ä¸‹ x86 æ±‡ç¼–å§ï¼Œç„¶åå‡ºä¸€ä¸ªåˆ©ç”¨ kvm ç»™åˆ«äººå†™ä¸€ä¸ªæ•™ç¨‹
3. åœ¨ kvm ä¸­é—´è¿è¡Œ unikernel ?
4. [^6] è™šæ‹ŸåŒ–å…¥é—¨ï¼Œå„ç§ hypervisor åˆ†ç±»
5. æˆ‘å¿ƒå¿ƒå¿µå¿µçš„ TLB åˆ‡æ¢åœ¨å“ªé‡Œå•Š ?
6. virtio çš„ä¸¤ä¸ªæ–‡ç« :
    1. https://www.redhat.com/en/blog/introduction-virtio-networking-and-vhost-net
    2. https://www.redhat.com/en/blog/virtio-devices-and-drivers-overview-headjack-and-phone
7. æ‰€ä»¥ kvm æ˜¯æ€ä¹ˆå’Œ virtio äº§ç”Ÿè”ç³»çš„ ï¼Ÿ
8. virtio å¦‚ä½•å¤„ç† GPU çš„ ?


## é—®é¢˜
- [ ] å¦‚æœ kvm ä¸­é—´è·‘ä¸€ä¸ªæ”¯æŒ multicore çš„ OSï¼Œkvm ä»ä¸€ä¸ª cpu ä¸­é—´å¯åŠ¨ï¼Œæœ€åå¯ä»¥è¿ç§»åˆ°å…¶ä»–çš„ CPU ä¸­é—´
    - [ ] å¦‚æœ kvm çš„ CPU æ•°é‡èƒ½å¦åŠ¨æ€æ‰©å±• ï¼Ÿ

## è®°å½•
[^1] lwn ç»™å‡ºäº†ä¸€ä¸ªè¶…çº§å…¥é—¨çš„ä»‹ç»ï¼Œå€¼å¾—å­¦ä¹  :

Each virtual CPU has an associated struct `kvm_run` data structure, 
used to communicate information about the CPU between the kernel and user space. 

he VCPU also includes the processor's register state, broken into two sets of registers: standard registers and "special" registers. These correspond to two architecture-specific data structures: `struct kvm_regs` and `struct kvm_sregs`, respectively. On x86, the standard registers include general-purpose registers, as well as the instruction pointer and flags; the "special" registers primarily include segment registers and control registers.

**This sample virtual machine demonstrates the core of the KVM API, but ignores several other major areas that many non-trivial virtual machines will care about.**

1. Prospective implementers of memory-mapped I/O devices will want to look at the `exit_reason` `KVM_EXIT_MMIO`, as well as the `KVM_CAP_COALESCED_MMIO` extension to reduce vmexits, and the `ioeventfd` mechanism to process I/O asynchronously without a vmexit.

2. For hardware interrupts, see the `irqfd` mechanism, using the `KVM_CAP_IRQFD` extension capability. This provides a file descriptor that can inject a hardware interrupt into the KVM virtual machine without stopping it first. A virtual machine may thus write to this from a separate event loop or device-handling thread, and threads running `KVM_RUN` for a virtual CPU will process that interrupt at the next available opportunity.

3. x86 virtual machines will likely want to support CPUID and model-specific registers (SRs), both of which have architecture-specific ioctl()s that minimize vmexits.M
> TODO è¿™å‡ ä¸ªè¿›é˜¶ï¼Œå€¼å¾—å…³æ³¨

While they can support other devices and `virtio` hardware, if you want to emulate a completely different type of system that shares little more than the instruction set architecture, you might want to implement a new VM instead. 

[^2]: é…ç½®çš„ä»£ç éå¸¸è¯¦å°½
TODO : å†…æ ¸åˆ‡æ¢åˆ° long mode çš„æ–¹æ³•æ¯”è¿™é‡Œå¤æ‚å¤šäº†, çœ‹çœ‹[devos](https://wiki.osev.org/Setting_Up_Long_Moded)

The two modes are distinguished by the `dpl` (descriptor privilege level) field in segment register `cs.dpl=3`  in `cs` for user-mode, and zero for kernel-mode (not sure if this "level" equivalent to so-called ring3 and ring0).

In real mode kernelshould handle the segment registers carefully, while in x86-64, instructions syscall and sysret will properly set segment registers automatically, so we don't need to maintain segment registers manually.


This is just an example, we should *NOT* set user-accessible pages in hypervisor, user-accessible pages should be handled by our kernel.
> è¿™äº›ä¾‹å­ `mv->mem` çš„å†…å­˜æ˜¯ hypervisor çš„ï¼Œåˆ°åº•ä»€ä¹ˆæ˜¯ hypervisor ?

Registration of syscall handler can be achieved via setting special registers named `MSR (Model Specific Registers)`. We can get/set MSR in hypervisor through `ioctl` on `vcpufd`, or in kernel using instructions `rdmsr` and `wrmsr`.

> å…¶å®ä»£ç çš„æ‰€æœ‰çš„ç»†èŠ‚åº”è¯¥è¢«ä»”ç»†çš„ç†è§£æ¸…æ¥š TODO
> 1. ç»å…¸çš„ while(1) å¾ªç¯ï¼Œç„¶åå¤„ç†å„ç§æƒ…å†µçš„ç»“æ„åœ¨å“ªé‡Œ
> 2. ä¼¼ä¹ç›´æ¥ä»‹ç»äº†å†…æ ¸çš„è¿è¡Œæ–¹å¼è€Œå·²


## container
kata å’Œ firecracker :


[^3] çš„è®°å½•ï¼Œclearcontainer åœæ­¢ç»´æŠ¤ï¼Œåªæ˜¯ä¸€ä¸ªå®£ä¼ çš„æ–‡ç« ï¼Œå…³äº memory overhead çš„ä½¿ç”¨ DAX æœ‰ç‚¹æ„æ€ã€‚


## virtio
é—®é¢˜ : 
2. åˆ©ç”¨ virtqueue è§£å†³äº†é«˜æ•ˆä¼ è¾“çš„æ•°æ®çš„é—®é¢˜ï¼Œé‚£ä¹ˆä¸­æ–­è™šæ‹ŸåŒ–æ€ä¹ˆåŠ ?


[^7] çš„è®°å½•:
åŠ¨æœº:
Linux supports 8 distinct virtualization systems:
- Xen, KVM, VMWare, ...
- Each of these has its own block, console, network, ... drivers

VirtIO â€“ The three goals
- Driver unification
- Uniformity to provide a common ABI for general publication and use of buffers
- Device probing and configuration

Virtqueue 
- It is a part of the memory of the
guest OS
- A channel between front-end and back-end
- It is an interface Implemented as
Vring 
  - Vring is a memory mapped region between QEMU and guest OS
  - Vring is the memory layout of the virtqueue abstraction




[^4] çš„è®°å½•:
The end goal of the process is to try to create a straightforward, efficient, and extensible standard.

- "Straightforward" implies that, to the greatest extent possible, devices should use existing bus interfaces. Virtio devices see something that looks like a standard PCI bus, for example; there is to be no "boutique hypervisor bus" for drivers to deal with. 
-  "Efficient" means that batching of operations is both possible and encouraged; interrupt suppression is supported, as is notification suppression on the device side. 
- "Extensible" is handled with feature bits on both the device and driver sides with a negotiation phase at device setup time; this mechanism, Rusty said, has worked well so far. And the standard defines a common ring buffer and descripor mechanism (a "virtqueue") that is used by all devices; the same devices can work transparently over different transports.
> changes for virtio 1.0 ä¹‹åæ²¡çœ‹ï¼Œå…ˆçœ‹ä¸ªæ›´åŠ ç®€å•çš„å§!

[^5] çš„è®°å½•:
Linux offers a variety of hypervisor solutions with different attributes and advantages. Examples include the Kernel-based Virtual Machine (KVM), lguest, and User-mode Linux
> @todo å¿½ç„¶ä¸çŸ¥é“ä»€ä¹ˆå«åš hypervisor äº†

Rather than have a variety of device emulation mechanisms (for network, block, and other drivers), virtio provides a common front end for these device emulations to standardize the interface and increase the reuse of code across the platforms.

> paravirtualization å’Œ virtualization çš„å…³ç³»
In the full virtualization scheme, the hypervisor must emulate device hardware, which is emulating at the lowest level of the conversation (for example, to a network driver). Although the emulation is clean at this abstraction, itâ€™s also the most inefficient and highly complicated. In the paravirtualization scheme, the guest and the hypervisor can work cooperatively to make this emulation efficient. The downside to the paravirtualization approach is that the operating system is aware that itâ€™s being virtualized and requires modifications to work.
![](https://developer.ibm.com/developer/articles/l-virtio/images/figure1.gif)

Here, the guest operating system is aware that itâ€™s running on a hypervisor and includes drivers that act as the front end. The hypervisor implements the back-end drivers for the particular device emulation. These front-end and back-end drivers are where virtio comes in, providing a standardized interface for the development of emulated device access to propagate code reuse and increase efficiency.

![](https://developer.ibm.com/developer/articles/l-virtio/images/figure2.gif)

> ä»£ç ç»“æ„
![](https://developer.ibm.com/developer/articles/l-virtio/images/figure4.gif)


Guest (front-end) drivers communicate with hypervisor (back-end) drivers through buffers. For an I/O, the guest provides one or more buffers representing the request.

Linking the guest driver and hypervisor driver occurs through the `virtio_device` and most commonly through `virtqueues`. The `virtqueue` supports its own API consisting of five functions. 
1. add_buf
2. kick
3. get_buf
4. enable_cb
5. disable_cb

> å…·ä½“çš„ä¾‹å­ : blk å¤§è‡´ 1000 è¡Œï¼Œnet å¤§è‡´ 3000 è¡Œï¼Œåœ¨ virtio ä¸­é—´å¤§è‡´ 6000 è¡Œ
You can find the source to the various front-end drivers within the ./drivers subdirectory of the Linux kernel. 
1. The virtio network driver can be found in ./drivers/net/virtio_net.c, and 
2. the virtio block driver can be found in ./drivers/block/virtio_blk.c. 
3. The subdirectory ./drivers/virtio provides the implementation of the virtio interfaces (virtio device, driver, virtqueue, and ring). 

## Intel VT-x
[wiki](https://en.wikipedia.org/wiki/X86_virtualization#Intel_virtualization_(VT-x))



https://github.com/cloudius-systems/osv/wiki/Running-OSv-image-under-KVM-QEMU : æœ‰æ„æ€ï¼Œå¯ä»¥æµ‹è¯•ä¸€ä¸‹
## å¾…å¤„ç†çš„èµ„æº
https://github.com/google/novm : å¿«é€Ÿå¼€å‘ï¼Œç„¶åå¿½ç„¶åœæ­¢, go è¯­è¨€å†™çš„ï¼Œ10000è¡Œå·¦å³



[^1]: https://lwn.net/Articles/658511/
[^2]: https://github.com/kvmtool/kvmtool
[^3]: [An Introduction to Clear Containers](https://lwn.net/Articles/644675/)
[^4]: [Standardizing virtio](https://lwn.net/Articles/580186/)
[^5]: https://developer.ibm.com/articles/l-virtio/
[^6]: https://developer.ibm.com/tutorials/l-hypervisor/
[^7]: https://www.cs.cmu.edu/~412/lectures/Virtio_2015-10-14.pdf
[^8]: https://david942j.blogspot.com/2018/10/noe-learning-kvm-implement-your-own.htmlt
[^9]: https://binarydebt.wordpress.com/201810/14/intel-virtualisation-how-vt-x-kvm-and-qemu-work-together//
[^10]: https://www.kernel.org/doc/html/latest/virt/kvm/index.html


# kvm

- [ ] put anything understand ./virt here

## åˆ†æä¸€ä¸‹
https://www.owalle.com/2019/02/20/kvm-src-analysis

å¾ªç¯ä¾èµ– ?
x86.c : å­˜æ”¾æ•´ä¸ª x86 é€šç”¨çš„å‡½æ•°ï¼Œemulate.c å’Œ vmx.c ä¸­é—´éƒ½ä¼šä½¿ç”¨çš„ä»£ç 
vmx.c : å¤„ç†å„ç§ exit çš„æ“ä½œ, å…¶ä¸­å¯èƒ½ä¼šè°ƒç”¨ emulate.c çš„é‚£å¤„ç†
emulate.c : å„ç§æŒ‡ä»¤çš„æ¨¡æ‹Ÿ


## å…³é”®çš„æ•°æ®ç»“æ„
```c
struct kvm_x86_ops // éš¾é“æ˜¯ä¸ºäº†å°† kvm_get_msr å¯¹äºä¸åŒçš„ x86 æ¶æ„ä¸Š ?

struct x86_emulate_ops // å®šä¹‰çš„å‡½æ•°éƒ½æ˜¯ç»™ emulate.c ä½¿ç”¨

struct vcpu_vmx {
	struct kvm_vcpu       vcpu;
  ...
}

/*
 * x86 supports 4 paging modes (5-level 64-bit, 4-level 64-bit, 3-level 32-bit,
 * and 2-level 32-bit).  The kvm_mmu structure abstracts the details of the
 * current mmu mode.
 */
struct kvm_mmu {
```


## TODO

```c
  kvm_mmu_gva_to_gpa_read:5516
  kvm_mmu_gva_to_gpa_fetch:5523
  kvm_mmu_gva_to_gpa_write:5531
  kvm_mmu_gva_to_gpa_system:5540
```
- [ ] https://www.cnblogs.com/ck1020/p/6920765.html å…¶ä»–çš„æ–‡ç« 

- [ ] https://www.kernel.org/doc/ols/2007/ols2007v1-pages-225-230.pdf
    - çœ‹çœ‹ KVM çš„æ€»ä½“ä¸Šå±‚æ¶æ„æ€ä¹ˆå›äº‹
- [ ] x86.c :  mmio / pio çš„å¤„ç†
- [ ] emulate.c ä¸­é—´æ¨¡æ‹Ÿçš„æŒ‡ä»¤æ•°é‡æ˜¾ç„¶æ˜¯è¿œè¿œæ²¡æœ‰è¾¾åˆ°å®é™…ä¸ŠæŒ‡ä»¤æ•°é‡çš„ï¼Œè€Œä¸”éƒ½æ˜¯å„ç§åŸºæœ¬æŒ‡ä»¤çš„æ¨¡æ‹Ÿ
  - [ ] ä¸ºä»€ä¹ˆè¦è¿›è¡Œè¿™äº›æ¨¡æ‹Ÿ, vmx çš„å„ç§ handle å‡½æ•°ä¸ºä»€ä¹ˆåè€Œä¸èƒ½å¤„ç†è¿™äº›ç®€å•çš„æŒ‡ä»¤
  - [ ] å¾ˆå¤šæ“ä½œä¾èµ–äº vcs read / write ï¼Œä½†æ˜¯è¿™é‡Œä»…ä»…æ˜¯åˆ©ç”¨ `ctxt->ops` ç„¶åè¯» vcpu ä¸­çš„å†…å®¹
- [ ] vcpu çš„ regs å’Œ vmcs çš„ regs çš„å…³ç³»æ˜¯ä»€ä¹ˆ ?
- [ ] cpuid.c ä¸ºä»€ä¹ˆæœ‰ 1000 è¡Œ,  kvm_emulate_cpuid  å’Œ ioctl API
- [ ] è°ƒæŸ¥ä¸€ä¸‹ kvm_vcpu_gfn_to_hva
- [x] kvm çš„ host va çš„åœ°å€åœ¨å“ªé‡Œ ? åœ¨ä½¿ç”¨ kvm çš„çº¿ç¨‹çš„ç”¨æˆ·ç©ºé—´ä¸­
- [ ] mmu å’Œ flush å’Œ zap æœ‰ä»€ä¹ˆåŒºåˆ« ?
- [ ] ept å’Œ shadow page table æ„Ÿè§‰å¤„ç†æ–¹æ³•ç±»ä¼¼äº†: éƒ½æ˜¯ for_each_shadow_entryï¼Œkvm_mmu_get_page, link_shadow_page å’Œ mmu_set_spte
    - [ ] `FNAME(fetch)`
    - [ ] `__direct_map`

- [ ] å¯¹äº shadow page table, ä¸åŒçš„ process éƒ½æœ‰ä¸€å¥—ï¼Œä¸åŒ process çš„ cr3 çš„åŠ è½½æ˜¯ä»€ä¹ˆæ—¶å€™ ?
- [ ] åœ¨ FNAME(page_fault) çš„ä¸¤ä¸ªæ­¥éª¤åˆ¤æ–­ï¼Œå½“è§£å†³äº† guest page table çš„é—®é¢˜ä¹‹åï¼Œä¾æ—§å‘ç”Ÿ page fault, æ­¤æ—¶æ·»åŠ ä¸Šçš„ shadow page table æ˜¾ç„¶å¯ä»¥ track ä¸Š
- [ ] dirty log



## å‡½æ•°è°ƒç”¨è·¯å¾„

```c
int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu) // x86.c
  static int vcpu_run(struct kvm_vcpu *vcpu) // x86.c
    static int vcpu_enter_guest(struct kvm_vcpu *vcpu) // x86.c
      static int vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath) // vmx.c
        static int (*kvm_vmx_exit_handlers[])(struct kvm_vcpu *vcpu) = { // vmx.c
          int __kvm_get_msr(struct kvm_vcpu *vcpu, u32 index, u64 *data, bool host_initiated)
            -> static int vmx_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
```
å½“ vmx è¿”å›å€¼å¤§äº 0 çš„æ—¶å€™ï¼Œä¼šå°†ç»“æœè¿”å›ç»™ç”¨æˆ·ç©ºé—´ï¼Œç”¨æˆ·ç©ºé—´å¤„ç†ã€‚



```c
gpa_t kvm_mmu_gva_to_gpa_read(struct kvm_vcpu *vcpu, gva_t gva,
			      struct x86_exception *exception)

// æœ€ç»ˆåœ¨ handle_exception_nmi
```


## x86.c overview
- VMCS çš„ IO
- timer pvclock tsc
- ioctl

- pio mmio å’Œ ä¸€èˆ¬çš„ IO çš„æ¨¡æ‹Ÿ
- emulate


1. debugfs
```c
static struct kmem_cache *x86_fpu_cache;
static struct kmem_cache *x86_emulator_cache;
```
2. kvm_on_user_return :
    1. user return ?
    2. share msr

3. exception_type

4. payload

æä¾›äº†å¾ˆå¤šå‡½æ•°è®¿é—®è®¾ç½® vcpuï¼Œæ¯”å¦‚ kvm_get_msr ä¹‹ç±»çš„
1. è°è°ƒç”¨ <- vmx.c å§ !
2. å®ç°çš„æ–¹æ³• : å°†å…¶æ”¾åœ¨ vmcs ä¸­ï¼Œ
ä» vmcs ä¸­é—´è¯»å– : å½“æƒ³è¦è®¿é—®çš„æ—¶å€™ï¼Œ

- [ ] vmcs æ˜¯å†…å­˜åŒºåŸŸï¼Œè¿˜ä¼šæ”¾åœ¨ CPU ä¸­é—´ï¼Œç”¨ æŒ‡ä»¤è¯»å†™çš„å†…å®¹

kvm_steal_time_set_preempted


## details

#### vmx_vcpu_run 
vmx_exit_handlers_fastpath : é€šè¿‡ omit what æ¥ fast


#### kvm_read_guest_virt_helper
å†…æ ¸è¯»å– guest çš„å†…å­˜ï¼Œå› ä¸º guest çš„ä½¿ç”¨åœ°å€ç©ºé—´æ˜¯
ç”¨æˆ·æ€çš„ï¼Œæ‰€ä»¥
1. gva_to_gpa çš„åœ°å€åˆ‡æ¢
		gpa_t gpa = vcpu->arch.walk_mmu->gva_to_gpa(vcpu, addr, access,
2. kvm_vcpu_read_guest_page : copy_to_user è€Œå·²




#### kvm_vcpu_ioctl_x86_set_mce 
å‘ guest æ³¨å…¥é”™è¯¯çš„æ–¹æ³•

kvm_queue_exception

So, what is bank ?


## event injection


#### kvm_vcpu_flush_tlb_all

```c
static void kvm_vcpu_flush_tlb_all(struct kvm_vcpu *vcpu)
{
	++vcpu->stat.tlb_flush;
	kvm_x86_ops.tlb_flush_all(vcpu);
}
```

## emulat.c
init_emulate_ctxt 
x86_emulate_instruction : 

```c
int kvm_emulate_instruction(struct kvm_vcpu *vcpu, int emulation_type)
{
	return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
}
```

1. emulate_ctxt çš„ä½¿ç”¨ä½ç½® :

	struct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;

- [x] emulate_ctxt.ops çš„è°ƒç”¨ä½ç½® ? åœ¨ emulate.c ä¸­é—´

1. ä¸ºä»€ä¹ˆä¼šå‡ºç° emulation_instruction çš„éœ€æ±‚ ?

```c
// å°† kvm_arch_vcpu_create è¢« kvm_vm_ioctl_create_vcpu å”¯ä¸€ call
int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
```

#### opcode_table çš„ä½¿ç”¨ä½ç½®

```c
static const struct opcode opcode_table[256] = {
```

æŒ‡ä»¤ç¼–ç :
```c
struct opcode {
	u64 flags : 56;
	u64 intercept : 8;
	union {
		int (*execute)(struct x86_emulate_ctxt *ctxt);
		const struct opcode *group;
		const struct group_dual *gdual;
		const struct gprefix *gprefix;
		const struct escape *esc;
		const struct instr_dual *idual;
		const struct mode_dual *mdual;
		void (*fastop)(struct fastop *fake);
	} u;
	int (*check_perm)(struct x86_emulate_ctxt *ctxt);
};
```

## direct_map
- [x] è¢«è°ƒç”¨è·¯å¾„: tdp çš„æ³¨å†Œå‡½æ•°
- [ ] åšä»€ä¹ˆçš„

kvm_tdp_page_fault
=> direct_page_fault : åˆ¶ä½œä¸€äº› cache
=> `__direct_map`

`__direct_map`
1. for_each_shadow_entry : åœ¨ tdp ä¸­é—´ä¸ºä»€ä¹ˆä¸ºä»€ä¹ˆå­˜åœ¨ shadow entry

#### `__direct_map`
1. for_each_shadow_entry : å› ä¸ºå¤šä¸ª shadow page æ˜ å°„ä¸€ä¸ª page table


## vmx.c

pt : https://lwn.net/Articles/741093/ : processor tracing

#### vmx_x86_ops

- struct x86_kvm_ops : vmx_x86_ops ä¹Ÿæ˜¯å…¶ä¸­ä¸€ç§
- x86_kvm_ops : ä¸€ä¸ªç»å¸¸è®¿é—®çš„å˜é‡


æä¾›å¯¹äº vmcs çš„æ ‡å‡†è®¿é—®ï¼Œå’Œ kvm_x86_ops çš„å…³ç³»æ˜¯ä»€ä¹ˆ ?

```c
static struct kvm_x86_init_ops vmx_init_ops __initdata = {
	.cpu_has_kvm_support = cpu_has_kvm_support,
	.disabled_by_bios = vmx_disabled_by_bios,
	.check_processor_compatibility = vmx_check_processor_compat,
	.hardware_setup = hardware_setup,

	.runtime_ops = &vmx_x86_ops,
};

// åœ¨ KVM init çš„æ—¶å€™ï¼Œç¡®å®šä½¿ç”¨ä½•ç§ç¡¬ä»¶è®¾ç½®ï¼Œä½†æ˜¯ emulate è¿˜æ˜¯å­˜åœ¨çš„
int kvm_arch_hardware_setup(void *opaque)
{
  // ...
	memcpy(&kvm_x86_ops, ops->runtime_ops, sizeof(kvm_x86_ops));
  // ...
```

## emulate_ops å’Œ vmx_x86_ops çš„æ“ä½œå¯¹æ¯”
- vmx_x86_ops æä¾›äº†å„ç§æ“ä½œçš„ç¡¬ä»¶æ”¯æŒ.
- vmx çš„ kvm_vmx_exit_handlers éœ€è¦ emulate çš„ï¼Œä½†æ˜¯ emulator çš„å·¥ä½œéœ€è¦ä» emulator ä¸­é—´å¾—åˆ°æ•°æ®



## hyperv.c
æ¨¡æ‹Ÿ HyperV çš„å†…å®¹, ä½†æ˜¯ä¸ºä»€ä¹ˆéœ€è¦æ¨¡æ‹Ÿ HyperV ?

- kvm_hv_hypercall
- stimer

å®åœ¨æ˜¯æœ‰ç‚¹çœ‹ä¸æ‡‚:
https://docs.microsoft.com/en-us/virtualization/hyper-v-on-windows/reference/hyper-v-architecture


## 8254 / 8259
KVM_CREATE_IRQCHIP :

https://en.wikipedia.org/wiki/Intel_8253

## irq.c
ä¼¼ä¹å¾ˆçŸ­ï¼Œä½†æ˜¯ lapic å¾ˆé•¿!


## ä¸­æ–­è™šæ‹ŸåŒ–
ä¸­æ–­è™šæ‹ŸåŒ–çš„å…³é”®åœ¨äºå¯¹ä¸­æ–­æ§åˆ¶å™¨çš„æ¨¡æ‹Ÿï¼Œæˆ‘ä»¬çŸ¥é“x86ä¸Šä¸­æ–­æ§åˆ¶å™¨ä¸»è¦æœ‰æ—§çš„ä¸­æ–­æ§åˆ¶å™¨PIC(intel 8259a)å’Œé€‚åº”äºSMPæ¡†æ¶çš„IOAPIC/LAPICä¸¤ç§ã€‚

https://luohao-brian.gitbooks.io/interrupt-virtualization/content/qemu-kvm-zhong-duan-xu-ni-hua-kuang-jia-fen-679028-4e2d29.html

æŸ¥è¯¢ GSI å·ä¸Šå¯¹åº”çš„æ‰€æœ‰çš„ä¸­æ–­å·:

ä» ioctl åˆ°ä¸‹å±‚ï¼Œkvm_vm_ioctl æ³¨å…¥çš„ä¸­æ–­ï¼Œæœ€åæ›´æ”¹äº† kvm_kipc_state:irr 

kvm_kipc_state çš„ä¿¡æ¯å¦‚ä½•å‘ŠçŸ¥ CPU ? é€šè¿‡ kvm_pic_read_irq

## lapic

#### https://luohao-brian.gitbooks.io/interrupt-virtualization/content/kvmzhi-nei-cun-xu-ni531628-kvm-mmu-virtualization.html

```c
struct kvm_memory_slot {
	gfn_t base_gfn;
	unsigned long npages;
	unsigned long *dirty_bitmap;
	struct kvm_arch_memory_slot arch;
	unsigned long userspace_addr;
	u32 flags;
	short id;
};

/*
 * Note:
 * memslots are not sorted by id anymore, please use id_to_memslot()
 * to get the memslot by its id.
 */
struct kvm_memslots {
	u64 generation;
	/* The mapping table from slot id to the index in memslots[]. */
	short id_to_index[KVM_MEM_SLOTS_NUM];
	atomic_t lru_slot;
	int used_slots;
	struct kvm_memory_slot memslots[];
};
```

`hva=base_hva+(gfn-base_gfn)*PAGE_SIZE`

```c
unsigned long gfn_to_hva(struct kvm *kvm, gfn_t gfn)
{
	return gfn_to_hva_many(gfn_to_memslot(kvm, gfn), gfn, NULL);
}

// å…³é”® : å®šä½ slot
struct kvm_memory_slot *gfn_to_memslot(struct kvm *kvm, gfn_t gfn)
{
	return __gfn_to_memslot(kvm_memslots(kvm), gfn);
}

// å®šä½ slot çš„æ ¸å¿ƒå‡½æ•°ï¼Œä¼°è®¡æ˜¯é¡ºç€æŸ¥è¯¢ä¸€éçš„æ ·å­
/*
 * search_memslots() and __gfn_to_memslot() are here because they are
 * used in non-modular code in arch/powerpc/kvm/book3s_hv_rm_mmu.c.
 * gfn_to_memslot() itself isn't here as an inline because that would
 * bloat other code too much.
 *
 * IMPORTANT: Slots are sorted from highest GFN to lowest GFN!
 */
static inline struct kvm_memory_slot *
search_memslots(struct kvm_memslots *slots, gfn_t gfn)
```

> ä½œç”¨ï¼šGVAç›´æ¥åˆ°HPAçš„åœ°å€ç¿»è¯‘,çœŸæ­£è¢«VMMè½½å…¥åˆ°ç‰©ç†MMUä¸­çš„é¡µè¡¨æ˜¯å½±å­é¡µè¡¨ï¼›
> MMU ä¼šåœ¨ mmu æ²¡æœ‰å‘½ä¸­çš„æ—¶å€™ crash

è·å¾—ç¼ºé¡µå¼‚å¸¸å‘ç”Ÿæ—¶çš„CR2,åŠå½“æ—¶è®¿é—®çš„è™šæ‹Ÿåœ°å€ï¼›
è¿›å…¥
```
kvm_mmu_page_fault()(vmx.c)->
r = vcpu->arch.mmu.page_fault(vcpu, cr2, error_code);(mmu.c)->
FNAME(page_fault)(struct kvm_vcpu *vcpu, gva_t addr, u32 error_code)(paging_tmpl.h)->
FNAME(walk_addr)() 
```
æŸ¥guesté¡µè¡¨ï¼Œç‰©ç†åœ°å€æ˜¯å¦å­˜åœ¨ï¼Œ è¿™æ—¶è‚¯å®šæ˜¯ä¸å­˜åœ¨çš„
The page is not mapped by the guest. Let the guest handle it.
`inject_page_fault()->kvm_inject_page_fault()` å¼‚å¸¸æ³¨å…¥æµç¨‹ï¼›

> åªè¦æ˜¯ mmu ä¸­é—´è®¿é—®å¤±è´¥éƒ½æ˜¯éœ€è¦è¿›è¡Œ vm exit çš„ï¼Œå¦‚æœå‘ç°æ˜¯ guest çš„é—®é¢˜ï¼Œé‚£ä¹ˆé€šçŸ¥ guest
> TODO æ‰¾åˆ°å¯¹äº guest çš„ page table è¿›è¡Œ walk çš„æ–¹æ³•
> Guest æå®šä¹‹åï¼Œé‚£ä¹ˆ
> TODO TLB çš„æŸ¥æ‰¾ä¸åˆ°ï¼Œè¢« VMM æˆªè·åº”è¯¥æ˜¯éœ€è¦ ç¡¬ä»¶æ”¯æŒçš„å§!

ä¸ºäº†å¿«é€Ÿæ£€ç´¢GUESTé¡µè¡¨æ‰€å¯¹åº”çš„çš„å½±å­é¡µè¡¨ï¼ŒKVM ä¸ºæ¯ä¸ªGUESTéƒ½ç»´æŠ¤äº†ä¸€ä¸ªå“ˆå¸Œ
è¡¨ï¼Œå½±å­é¡µè¡¨å’ŒGUESTé¡µè¡¨é€šè¿‡æ­¤å“ˆå¸Œè¡¨è¿›è¡Œæ˜ å°„ã€‚å¯¹äºæ¯ä¸€ä¸ªGUESTæ¥è¯´ï¼ŒGUEST
çš„é¡µç›®å½•å’Œé¡µè¡¨éƒ½æœ‰å”¯ä¸€çš„GUESTç‰©ç†åœ°å€ï¼Œé€šè¿‡é¡µç›®å½•/é¡µè¡¨çš„å®¢æˆ·æœºç‰©ç†åœ°å€å°±
å¯ä»¥åœ¨å“ˆå¸Œé“¾è¡¨ä¸­å¿«é€Ÿåœ°æ‰¾åˆ°å¯¹åº”çš„å½±å­é¡µç›®å½•/é¡µè¡¨ã€‚
> æ˜¾ç„¶ä¸å¯èƒ½ä½¿ç”¨ä¿å­˜æ‰€æœ‰çš„ç‰©ç†åœ°å€ï¼Œä»è™šæ‹Ÿæœºåªä¼šå°†è™šæ‹Ÿæœºä½¿ç”¨çš„ç‰©ç†åœ°å€å¤„ç†æ‰

> å¡«å……è¿‡ç¨‹

mmu_alloc_root =>
`__direct_map` => kvm_mmu_get_page =>


æ„Ÿè§‰è¿™é‡Œè¿˜æ˜¯ shadow çš„å¤„ç†æœºåˆ¶ï¼Œé‚£ä¹ˆ ept åœ¨å“ªé‡Œ ?
```c
static int __direct_map(struct kvm_vcpu *vcpu, gpa_t gpa, int write,
			int map_writable, int max_level, kvm_pfn_t pfn,
			bool prefault, bool account_disallowed_nx_lpage)
{
  // TODO æ˜¯åœ¨å¯¹äºè°è¿›è¡Œ walk ? åº”è¯¥ä¸æ˜¯æ˜¯å¯¹äº shadow page è¿›è¡Œçš„
  // shadow page ä¹Ÿæ˜¯åˆ’åˆ†ä¸º leaf å’Œ nonleaf çš„ï¼Œä¹Ÿå°±æ˜¯è¿™æ˜¯å¯¹äº shadow çš„
  // 
  // shadow page å½¢æˆä¸€ä¸ªå±‚æ¬¡ç»“æ„çš„ç›®çš„æ˜¯ä»€ä¹ˆ ?
	struct kvm_shadow_walk_iterator it;
	struct kvm_mmu_page *sp;
	int level, ret;
	gfn_t gfn = gpa >> PAGE_SHIFT;
	gfn_t base_gfn = gfn;

	if (WARN_ON(!VALID_PAGE(vcpu->arch.mmu->root_hpa)))
		return RET_PF_RETRY;

  // TODO level generation çš„å«ä¹‰
  // level : éš¾é“ shadow page table ä¹Ÿæ˜¯éœ€è¦å¤šä¸ª level
	level = kvm_mmu_hugepage_adjust(vcpu, gfn, max_level, &pfn);

	for_each_shadow_entry(vcpu, gpa, it) {
		/*
		 * We cannot overwrite existing page tables with an NX
		 * large page, as the leaf could be executable.
		 */
		disallowed_hugepage_adjust(it, gfn, &pfn, &level);

		base_gfn = gfn & ~(KVM_PAGES_PER_HPAGE(it.level) - 1);
		if (it.level == level)
			break;

		drop_large_spte(vcpu, it.sptep);
		if (!is_shadow_present_pte(*it.sptep)) {
			sp = kvm_mmu_get_page(vcpu, base_gfn, it.addr,
					      it.level - 1, true, ACC_ALL);

			link_shadow_page(vcpu, it.sptep, sp);
			if (account_disallowed_nx_lpage)
				account_huge_nx_page(vcpu->kvm, sp);
		}
	}

	ret = mmu_set_spte(vcpu, it.sptep, ACC_ALL,
			   write, level, base_gfn, pfn, prefault,
			   map_writable);
	direct_pte_prefetch(vcpu, it.sptep);
	++vcpu->stat.pf_fixed;
	return ret;
}
```
==> kvm_mmu_get_page : åº”è¯¥ä¿®æ”¹ä¸º get_shadow_page
==> kvm_page_table_hashfn : åˆ©ç”¨ gfn ä½œä¸º hash å¿«é€Ÿå®šä½ shadow_page
==> kvm_mmu_alloc_page : åˆ†é…å¹¶ä¸”åˆå§‹åŒ–ä¸€ä¸ª shadow page table

æ³¨æ„ : shadow page table ä¼¼ä¹å¯ä»¥å­˜æ”¾ shadow page table entry çš„

**TODO** è°ƒæŸ¥ kvm_mmu_alloc_page çš„åˆ›å»ºçš„ kvm_mmu_page çš„ç®¡ç†å†…å®¹, ä¼¼ä¹ rule è¯´æ˜äº†å¾ˆå¤šä¸œè¥¿

The hypervisor computes the guest virtual to
host physical mapping on the fly and stores it in
a new set of page tables

https://www.linux-kvm.org/images/e/e5/KvmForum2007%24shadowy-depths-of-the-kvm-mmu.pdfhttps://www.linux-kvm.org/images/e/e5/KvmForum2007%24shadowy-depths-of-the-kvm-mmu.pdf

emmmm : ä¸€ä¸ªç‰©ç†é¡µé¢ï¼Œåœ¨ host çœ‹æ¥æ˜¯ç»™ host ä½¿ç”¨çš„ï¼Œwrite protect  å¯ä»¥åœ¨ guest ä¸­é—´ï¼Œ
ä¹Ÿæ˜¯å¯ä»¥æ”¾åœ¨ host ä¸­é—´ã€‚

emmmm : ä»€ä¹ˆæƒ…å†µä¸‹ï¼Œä¸€ä¸ª hva å¯ä»¥è¢«å¤šä¸ª gpa æ˜ å°„ ?

å¯¹äº guest çš„é‚£äº› page tableï¼Œéœ€è¦é€šè¿‡ `page->private` å…³è”èµ·æ¥.

- When we shadow a guest page, we iterate over
the reverse map and remove write access

- When adding write permission to a page, we
check whether the page has a shadow

- **We can have multiple shadow pages for a
single guest page â€“ one for each role**

#### shadow page descriptor
TODO : shadow page table åœ¨ TLB miss çš„æ—¶å€™ï¼Œè§¦å‘ exception å— ?

- [x] æ—¢ç„¶ hash table å¯ä»¥æŸ¥è¯¢ï¼Œä¸ºä»€ä¹ˆè¿˜è¦å»ºç«‹ hierarchy çš„ shadow page table ?
- [x] hash page table ä¸­é—´æ”¾ç½®æ‰€æœ‰çš„ä» gva åˆ° hpa çš„åœ°å€ ?

- å»ºç«‹ hash æ˜¯ä¸ºäº†è®© guest çš„ page table å’Œ host çš„ shadow page table ä¹‹é—´å¯ä»¥å¿«é€ŸæŸ¥æ‰¾.
- shadow page table : gva åˆ° hpa çš„æ˜ å°„ï¼Œè¿™ä¸ªæ˜ å°„æ˜¯ä¸€ä¸ª tree çš„ç»“æ„


## sync shadow page
1. åˆ©ç”¨ generation æ¥å®ç°å®šä½ ?

```c
static bool is_obsolete_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
{
	return sp->role.invalid ||
	       unlikely(sp->mmu_valid_gen != kvm->arch.mmu_valid_gen);
}
```

## trace mmu


## mmu_spte_update
TODO : ä¸ºä»€ä¹ˆä¼šå­˜åœ¨ä¸€ä¸ª writable spte å’Œ read-only spte çš„åŒºåˆ† ?

```c
/* Rules for using mmu_spte_update:
 * Update the state bits, it means the mapped pfn is not changed.
 *
 * Whenever we overwrite a writable spte with a read-only one we
 * should flush remote TLBs. Otherwise rmap_write_protect
 * will find a read-only spte, even though the writable spte
 * might be cached on a CPU's TLB, the return value indicates this
 * case.
 *
 * Returns true if the TLB needs to be flushed
 */
static bool mmu_spte_update(u64 *sptep, u64 new_spte)
```

æ ¸å¿ƒå°±æ˜¯ WRITE_ONCE è€Œå·²ï¼Œä½†æ˜¯å­˜åœ¨å¾ˆå¤šæ£€æŸ¥

## ept 

tdp_page_fault()->
gfn_to_pfn(); GPAåˆ°HPAçš„è½¬åŒ–åˆ†ä¸¤æ­¥å®Œæˆï¼Œåˆ†åˆ«é€šè¿‡gfn_to_hvaã€hva_to_pfnä¸¤ä¸ªå‡½æ•°å®Œæˆ
`__direct_map()`; å»ºç«‹EPTé¡µè¡¨ç»“æ„

ä¸ºä»€ä¹ˆ ept ä¹Ÿæ˜¯éœ€è¦å»ºç«‹ä¸€ä¸ª shadow page table ?


kvm_tdp_page_fault å’Œ ept_page_fault çš„å…³ç³»æ˜¯ä»€ä¹ˆ ?

## paging_tmpl.h

We need the mmu code to access both 32-bit and 64-bit guest ptes,
so the code in this file is compiled twice, once per pte size.

- [x] å¦‚ä½•å®ç°å¤šæ¬¡ç¼–è¯‘ ? ç›®çš„åº”è¯¥æ˜¯æä¾›ä¸‰ç§ä¸åŒç¼–è¯‘å±æ€§çš„æ–‡ä»¶ï¼Œå…¶ä¸­åªæ˜¯å°‘é‡åç§»é‡çš„ä¿®æ”¹ã€‚é€šè¿‡ä¸‰æ¬¡ include è§£å†³.
- [ ] å¦‚æœ guest ä½¿ç”¨ transparent huge page çš„æ—¶å€™ï¼Œå…¶æä¾›çš„ page walk æ€ä¹ˆåŠ ?


```c
static void shadow_mmu_init_context(struct kvm_vcpu *vcpu, struct kvm_mmu *context,
				    u32 cr0, u32 cr4, u32 efer,
				    union kvm_mmu_role new_role)
{
	if (!(cr0 & X86_CR0_PG))
		nonpaging_init_context(vcpu, context);
	else if (efer & EFER_LMA)
		paging64_init_context(vcpu, context);
	else if (cr4 & X86_CR4_PAE)
		paging32E_init_context(vcpu, context);
	else
		paging32_init_context(vcpu, context);

	context->mmu_role.as_u64 = new_role.as_u64;
	reset_shadow_zero_bits_mask(vcpu, context);
}
```
> éƒ½æ˜¯æä¾›çš„ shadow çš„æƒ…å†µï¼Œé‚£ä¹ˆ ept å’Œ tdp æ‰€ä»¥æ²¡æœ‰å‡ºç° ?

## shadow page table
- [ ] shadow page table æ˜¯æ”¾åœ¨ qemu çš„ç©ºé—´ä¸­é—´ï¼Œè¿˜æ˜¯å†…æ ¸åœ°å€ç©ºé—´
  - guest é€šè¿‡ cr3 å¯ä»¥æ¥è®¿é—®
  - å†…æ ¸å¯ä»¥æ“æ§ page table
- [ ] guest çš„å†…æ ¸ vmalloc ä¿®æ”¹ page tableï¼Œæ˜¯é¦–å…ˆä¿®æ”¹ shadow page table é€ æˆçš„å¼‚å¸¸ï¼Œç„¶åä¹‹åæ‰ä¿®æ”¹ guest page table ?
    - [ ] shadow page table å„ä¸ªçº§åˆ«å­˜æ”¾çš„åœ°å€æ˜¯ä»€ä¹ˆ ? ç‰©ç†åœ°å€ï¼Œå› ä¸ºæ˜¯è®© cr3 ä½¿ç”¨çš„
    - [x] guest page table çš„å†…å®¹ ? GVA ä¹Ÿå°±æ˜¯ host çš„è™šæ‹Ÿåœ°å€
- [x] `FNAME(walk_addr)()` å­˜å‚¨çš„åœ°å€éƒ½æ˜¯ guest çš„è™šæ‹Ÿåœ°å€ ? æ˜¯çš„ï¼Œæ‰€ä»¥åº”è¯¥å¾ˆå®¹æ˜“ walk.

> FNAME(walk_addr)() æŸ¥ guesté¡µè¡¨ï¼Œç‰©ç†åœ°å€æ˜¯å¦å­˜åœ¨ï¼Œè¿™æ—¶è‚¯å®šæ˜¯ä¸å­˜åœ¨çš„
`inject_page_fault()->kvm_inject_page_fault()` å¼‚å¸¸æ³¨å…¥æµç¨‹ï¼›

åœ¨ Host ä¸­é—´æ£€æŸ¥å‘ç°ä¸å­˜åœ¨ï¼Œç„¶ååœ¨ä½¿ç”¨ inject pg åˆ° guest.
å› ä¸º guest page table å­˜åœ¨å¤šä¸ªæ¨¡å‹

è®© Host è¶Šä¿ä»£åº–æ¥èµ°ä¸€é guest çš„ page walkï¼Œshadow page table æ˜¯ CR3 ä¸­é—´å®é™…ä½¿ç”¨çš„ page table.
-> ä½¿ç”¨ spt ï¼Œå‡ºç° exception æ˜¯ä¸çŸ¥é“åˆ°åº•å“ªä¸€ä¸ªå±‚æ¬¡å‡ºç°é—®é¢˜çš„, æ‰€ä»¥éƒ½æ˜¯éœ€è¦æŠ›å‡ºæ¥æ£€æŸ¥çš„
-> *é‚£ä¹ˆå½“ guest é€šè¿‡ cr3 è¿›è¡Œä¿®æ”¹ shadow page table çš„æ—¶å€™ï¼Œé€šè¿‡ write protection å¯ä»¥æ‰¾åˆ° ?*
-> *å¥½åƒ shadow page åªèƒ½å­˜æ”¾ 512 ä¸ª page table entry,  åˆ©ç”¨ cr3 è®¿é—®çœŸçš„æ²¡æœ‰é—®é¢˜å— ?*

> å½±å­é¡µè¡¨åˆæ˜¯è½½å…¥åˆ°CR3ä¸­çœŸæ­£ä¸ºç‰©ç†MMUæ‰€åˆ©ç”¨è¿›è¡Œå¯»å€çš„é¡µè¡¨ï¼Œå› æ­¤å¼€å§‹æ—¶ä»»ä½•çš„å†…å­˜è®¿é—®æ“ä½œéƒ½ä¼šå¼•èµ·ç¼ºé¡µå¼‚å¸¸ï¼›å¯¼è‡´vmå‘ç”ŸVM Exitï¼›è¿›å…¥handle_exception();

## ept page table
- [ ] ept å’Œ shadow page table ä¸åº”è¯¥å…±äº«ç»“æ„å•Š

shadow page table : gva => hpa
ept : åº”è¯¥æ˜¯ GPA åˆ° HPA

- init_kvm_tdp_mmu
- kvm_mmu_alloc_page  : ç”³è¯· kvm_mmu_page ç©ºé—´ï¼Œè¯¥ç»“æ„è¡¨ç¤º EPT é¡µè¡¨é¡¹
- vmx_load_mmu_pgd : ä¼ å…¥çš„root_hpaä¹Ÿå°±ç›´æ¥å½“Guest CR3ç”¨ï¼Œå…¶å®å°±æ˜¯å½±å­é¡µè¡¨çš„åŸºå€ã€‚

- å½“CPUè®¿é—®EPTé¡µè¡¨æŸ¥æ‰¾HPAæ—¶ï¼Œå‘ç°ç›¸åº”çš„é¡µè¡¨é¡¹ä¸å­˜åœ¨ï¼Œåˆ™ä¼šå‘ç”ŸEPT Violationå¼‚å¸¸ï¼Œå¯¼è‡´VM-Exit

**GPAåˆ°HPAçš„æ˜ å°„å…³ç³»ç”±EPTé¡µè¡¨æ¥ç»´æŠ¤**

## ept å’Œ shadow page table ä¸­é—´çš„å†…å®¹
- ept å’Œ shadow page table çš„æ ¼å¼ç›¸åŒï¼Œè®©ç¡¬ä»¶è®¿é—®å¯ä»¥æ ¼å¼ç›¸åŒ
- ç»´æŠ¤ ept æ˜¯ä½¿ç”¨è½¯ä»¶çš„æ–¹æ³•ç»´æŠ¤çš„ï¼Œé‚£ä¹ˆ ept éƒ½æ˜¯ç‰©ç†åœ°å€

pgd : page global directory


kvm_mmu_load_pgd : `vcpu->arch.mmu->root_hpa` ä½œä¸ºå‚æ•°ä¼ é€’å‡ºå»

kvm_init_mmu : å¤„ç†ä¸‰ç§ mmu åˆå§‹åŒ–
  -> init_kvm_softmmu : shadow
  -> init_kvm_tdp_mmu

## æ‰¾åˆ° shadow ä»¥åŠ ept çš„ page table entry


## mmu_alloc_root
è°ƒç”¨ kvm_mmu_get_pageï¼Œä½†æ˜¯å…¶åˆ©ç”¨ hash æ¥æŸ¥æ‰¾ï¼Œè¯´å¥½çš„ hash æ˜¯ç”¨äº id çš„å•Š

## arch.mmu->root_hpa å’Œ arch.mmu->root_pgd
- [x] æ˜¯ä¸æ˜¯ root_hpa è¢« direct ä½¿ç”¨ï¼Œroot_pgd è¢« shadow ä½¿ç”¨
  - å¹¶ä¸æ˜¯ï¼Œéƒ½ä¾èµ–äº hpa è¿›è¡Œ page walkï¼Œè€Œ root_pgd å°±æ˜¯ guest cr3 çš„å€¼ï¼Œè¿™æ˜¯ GPA


mmu_alloc_shadow_roots : `root_pgd = vcpu->arch.mmu->get_guest_pgd(vcpu);`
mmu_alloc_direct_roots : root_pgd = 0


get_guest_pgd çš„ä¸€èˆ¬æ³¨å†Œå‡½æ•°:
```c
static unsigned long get_cr3(struct kvm_vcpu *vcpu)
{
	return kvm_read_cr3(vcpu);
}

// è¯»å– cr3 ä¼¼ä¹ä¸æ˜¯ä¸€å®šä¼šä» vmcs ä¸­é—´è¯»å–
static inline ulong kvm_read_cr3(struct kvm_vcpu *vcpu)
{
	if (!kvm_register_is_available(vcpu, VCPU_EXREG_CR3))
		kvm_x86_ops.cache_reg(vcpu, VCPU_EXREG_CR3);
	return vcpu->arch.cr3;
}
```




1. `arch.mmu->root_hpa` çš„åˆå§‹åŒ–

mmu_alloc_direct_roots
```c
static int mmu_alloc_roots(struct kvm_vcpu *vcpu)
{
	if (vcpu->arch.mmu->direct_map)
		return mmu_alloc_direct_roots(vcpu);
	else
		return mmu_alloc_shadow_roots(vcpu);
}
```
## memory in kernel or qumu process
luohao's blog:

- [ ] rmap å­—æ®µçš„è§£é‡Šï¼Œé‚£ä¹ˆ memory æ˜¯ vmalloc åˆ†é…çš„ ?????
  - [ ] vmalloc çš„åˆ†é…æ˜¯ page fault çš„å— ?

```c
struct kvm_memory_slot {
    gfn_t base_gfn;                    // è¯¥å—ç‰©ç†å†…å­˜å—æ‰€åœ¨guest ç‰©ç†é¡µå¸§å·
    unsigned long npages;              //  è¯¥å—ç‰©ç†å†…å­˜å—å ç”¨çš„pageæ•°
    unsigned long flags;
    unsigned long *rmap;               // åˆ†é…è¯¥å—ç‰©ç†å†…å­˜å¯¹åº”çš„hostå†…æ ¸è™šæ‹Ÿåœ°å€ï¼ˆvmallocåˆ†é…ï¼‰
    unsigned long *dirty_bitmap;
    struct {
        unsigned long rmap_pde;
        int write_count;
    } *lpage_info[KVM_NR_PAGE_SIZES - 1];
    unsigned long userspace_addr;       // ç”¨æˆ·ç©ºé—´åœ°å€ï¼ˆQEMU)
    int user_alloc;
};
```

## rmap
https://www.cnblogs.com/ck1020/p/6920765.html

åœ¨KVMä¸­ï¼Œé€†å‘æ˜ å°„æœºåˆ¶çš„ä½œç”¨æ˜¯ç±»ä¼¼çš„ï¼Œä½†æ˜¯å®Œæˆçš„å´ä¸æ˜¯ä»HPAåˆ°å¯¹åº”çš„EPTé¡µè¡¨é¡¹çš„å®šä½ï¼Œ
è€Œæ˜¯ä»gfnåˆ°*å¯¹åº”çš„é¡µè¡¨é¡¹*çš„å®šä½ã€‚
*ç†è®ºä¸Šè®²æ ¹æ®gfnä¸€æ­¥æ­¥éå†EPTä¹Ÿæœªå°ä¸å¯ï¼Œä½†æ˜¯æ•ˆç‡è¾ƒä½*å†µä¸”åœ¨EPTæ‰€ç»´æŠ¤çš„é¡µé¢ä¸åŒäºhostçš„é¡µè¡¨ï¼Œ*ç†è®ºä¸Šè®²æ˜¯è™šæ‹Ÿæœºä¹‹é—´æ˜¯ç¦æ­¢ä¸»åŠ¨çš„å…±äº«å†…å­˜çš„*ï¼Œä¸ºäº†æé«˜æ•ˆç‡ï¼Œå°±æœ‰äº†å½“å‰çš„é€†å‘æ˜ å°„æœºåˆ¶ã€‚

- rmap: from guest page to shadow ptes that map it
- Shadow hash: from guest page to its shadow
- Parent pte chain: from shaow page to upperlevel shadow page
- Shadow pte: from shadow page to lower-level shadow page
- LRU: all active shadow pages

Walk the shadow page table, instantiating page tables as necessary
- Can involve an rmap walk and *write protecting the guest page table*


```c
struct kvm_arch_memory_slot {
  // åº”è¯¥æ˜¯ä¸€ç§ page size ç„¶åæä¾›ä¸€ç§ rmap å§
	struct kvm_rmap_head *rmap[KVM_NR_PAGE_SIZES];
	struct kvm_lpage_info *lpage_info[KVM_NR_PAGE_SIZES - 1];
	unsigned short *gfn_track[KVM_PAGE_TRACK_MAX];
};

#define KVM_MAX_HUGEPAGE_LEVEL	PG_LEVEL_1G
#define KVM_NR_PAGE_SIZES	(KVM_MAX_HUGEPAGE_LEVEL - PG_LEVEL_4K + 1)

enum pg_level {
	PG_LEVEL_NONE,
	PG_LEVEL_4K,
	PG_LEVEL_2M,
	PG_LEVEL_1G,
	PG_LEVEL_512G,
	PG_LEVEL_NUM
};
```

```c
static int kvm_alloc_memslot_metadata(struct kvm_memory_slot *slot,
				      unsigned long npages)
    // æ¯ä¸€ä¸ª page éƒ½ä¼šå»ºç«‹ä¸€ä¸ª
		slot->arch.rmap[i] =
			kvcalloc(lpages, sizeof(*slot->arch.rmap[i]),
    // ....
}

// mmu_set_spte çš„åœ°æ–¹è°ƒç”¨
static int rmap_add(struct kvm_vcpu *vcpu, u64 *spte, gfn_t gfn)
{
	struct kvm_mmu_page *sp;
	struct kvm_rmap_head *rmap_head;

  // é€šè¿‡ pte çš„æŒ‡é’ˆï¼Œè·å– spte æŒ‡å‘çš„ pte æ‰€åœ¨çš„ page çš„
	sp = sptep_to_sp(spte);
  // shadow å’Œ direct éƒ½æ˜¯éœ€è¦ rmap
  // ä½†æ˜¯ï¼Œdirect å…¶å®å¹¶ä¸ä¼šæ³¨å†Œ
	kvm_mmu_page_set_gfn(sp, spte - sp->spt, gfn);
	rmap_head = gfn_to_rmap(vcpu->kvm, gfn, sp);
	return pte_list_add(vcpu, spte, rmap_head);
}
```

```c
static gfn_t kvm_mmu_page_get_gfn(struct kvm_mmu_page *sp, int index)
{
	if (!sp->role.direct)
		return sp->gfns[index];

  // TODO guest çš„ç‰©ç†é¡µé¢åº”è¯¥å°±æ˜¯è¿ç»­çš„å•Š!
  // å½“ level åœ¨æœ€åº•å±‚çš„æ—¶å€™ï¼Œsp->gfn + index å°±å¯ä»¥äº†å•Š!
	return sp->gfn + (index << ((sp->role.level - 1) * PT64_LEVEL_BITS));
}


static struct kvm_rmap_head *gfn_to_rmap(struct kvm *kvm, gfn_t gfn,
					 struct kvm_mmu_page *sp)
{
	struct kvm_memslots *slots;
	struct kvm_memory_slot *slot;

	slots = kvm_memslots_for_spte_role(kvm, sp->role);
	slot = __gfn_to_memslot(slots, gfn);
	return __gfn_to_rmap(gfn, sp->role.level, slot);
}
```


- [ ] å»ºç«‹åå‘æ˜ å°„çš„åŸå› æ˜¯ : å½“ shadow page table è¿›è¡Œä¿®æ”¹ä¹‹åï¼Œéœ€è¦çŸ¥é“å…¶æ‰€åœ¨çš„ gfn
  - [ ] çœŸçš„å­˜åœ¨æ ¹æ® shadow page table åˆ° gfn çš„éœ€æ±‚å— ?
- [ ] direct éœ€è¦ rmap å— ? æ˜¾ç„¶éœ€è¦ï¼Œä¸ç„¶ direct_map ä¸ä¼šè°ƒç”¨ rmap_add


```c
	kvm_mmu_page_set_gfn(sp, spte - sp->spt, gfn); // ä¸€ä¸ª shadow page å’Œ gfn çš„å…³ç³»
	rmap_head = gfn_to_rmap(vcpu->kvm, gfn, sp);
	return pte_list_add(vcpu, spte, rmap_head); // slot çš„æ¯ä¸€ä¸ª page éƒ½ä¼šè¢« rmap
```

å®é™…ä¸Šï¼Œå­˜åœ¨ä¸¤ä¸ª rmap 
- `sp->gfns` è·å–æ¯ä¸€ä¸ª pte å¯¹åº”çš„ gfn
- `rmap_head->val` = spte : è¿™ä¸æ˜¯ rmap å§

#### parent rmap
```c
static void mmu_page_add_parent_pte(struct kvm_vcpu *vcpu,
				    struct kvm_mmu_page *sp, u64 *parent_pte)
{
	if (!parent_pte)
		return;

	pte_list_add(vcpu, parent_pte, &sp->parent_ptes);
}
```

#### rmap iterator
- [x] rmap æ€»æ˜¯æ„å»ºçš„ rmap_head åˆ° sptep å— ?
  - rmap_add å’Œ mmu_page_add_parent_pte éƒ½æ˜¯çš„

è§£æ for_each_rmap_spte
```c
#define for_each_rmap_spte(_rmap_head_, _iter_, _spte_)			\
	for (_spte_ = rmap_get_first(_rmap_head_, _iter_);		\
	     _spte_; _spte_ = rmap_get_next(_iter_))
```
ä½¿ç”¨ä½ç½®: 
kvm_mmu_write_protect_pt_masked : ç»™å®š gfn_offsetï¼Œå°†å…³è”çš„æ‰€æœ‰çš„ spte å…¨éƒ¨æ·»åŠ  flags

kvm_set_pte_rmapp : å°† rmap_head çš„æŒæœ‰çš„æ‰€æœ‰çš„ sptep è¿›è¡Œè®¾ç½®



## parent_ptes
```c
static void kvm_mmu_mark_parents_unsync(struct kvm_mmu_page *sp)
{
	u64 *sptep;
	struct rmap_iterator iter;

	for_each_rmap_spte(&sp->parent_ptes, &iter, sptep) {
		mark_unsync(sptep);
	}
}

static void mark_unsync(u64 *spte)
{
	struct kvm_mmu_page *sp;
	unsigned int index;

	sp = sptep_to_sp(spte);
	index = spte - sp->spt;
	if (__test_and_set_bit(index, sp->unsync_child_bitmap))
		return;
	if (sp->unsync_children++)
		return;
	kvm_mmu_mark_parents_unsync(sp);
}
```
é€’å½’å‘ä¸Šï¼Œå½“å‘ç°å­˜åœ¨æœ‰äºº æ²¡æœ‰ unsync çš„æ—¶å€™ï¼Œåœ¨ unsync_child_bitmap ä¸­é—´è®¾ç½®æ ‡å¿—ä½ï¼Œ
å¹¶ä¸”å‘ä¸Šä¼ å¯¼ï¼Œç›´åˆ°å‘ç°æ²¡äººæ£€æµ‹è¿‡

link_shadow_page : mark_unsync çš„å”¯ä¸€è°ƒç”¨ä½ç½®
kvm_unsync_page : kvm_mmu_mark_parents_unsync å”¯ä¸€è°ƒç”¨ä½ç½®

mmu_need_write_protect : å¯¹äºsp 

#### mmu_need_write_protect
for_each_gfn_indirect_valid_sp : ä¸€ä¸ª gfn å¯ä»¥
åŒæ—¶å¯¹åº”å¤šä¸ª shadow pageï¼ŒåŸå› æ˜¯ä¸€ä¸ª guest page å¯ä»¥å¯¹åº”å¤šä¸ª shadow page


> hash : å®ç° guest page tabel å’Œ shadow page çš„æ˜ å°„

> rmap_add å¤„ç†çš„æ˜¯ :  gfn å’Œå…¶å¯¹åº”çš„ pte çš„å¯¹åº”å…³ç³»


## role.quadrant
ä½œç”¨: ä¸€ä¸ª guest åœ°å€å¯¹åº”çš„ page table

get_written_sptes : ä¾é  gpa çš„ page_offset è®¡ç®—å‡ºæ¥ï¼Œç„¶åå’Œ `sp->role.quadrant` å¯¹æ¯”

#### obsolete sp

```c
static bool is_obsolete_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
{
	return sp->role.invalid ||
	       unlikely(sp->mmu_valid_gen != kvm->arch.mmu_valid_gen);
}
```

#### gfn_to_rmap
RMAP_RECYCLE_THRESHOLD å±…ç„¶æ˜¯ 1000

## gfn_track

```diff
 History:        #0
 Commit:         3d0c27ad6ee465f174b09ee99fcaf189c57d567a
 Author:         Xiao Guangrong <guangrong.xiao@linux.intel.com>
 Committer:      Paolo Bonzini <pbonzini@redhat.com>
 Author Date:    Wed 24 Feb 2016 09:51:11 AM UTC
 Committer Date: Thu 03 Mar 2016 01:36:21 PM UTC

 KVM: MMU: let page fault handler be aware tracked page

 The page fault caused by write access on the write tracked page can not
 be fixed, it always need to be emulated. page_fault_handle_page_track()
 is the fast path we introduce here to skip holding mmu-lock and shadow
 page table walking

 However, if the page table is not present, it is worth making the page
 table entry present and readonly to make the read access happy

 mmu_need_write_protect() need to be cooked to avoid page becoming writable
 when making page table present or sync/prefetch shadow page table entries

 Signed-off-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
 Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
```
-  [ ] tracked çš„ page ä¸èƒ½è¢« fixed, å¿…é¡»è¢«æ¨¡æ‹Ÿï¼Œä¸ºå•¥ ?

gfn_track å…¶å®æ²¡æœ‰ä»€ä¹ˆç‰¹åˆ«çš„ï¼Œå‘Šè¯‰è¯¥ é¡µé¢è¢« track äº†ï¼Œç„¶å
kvm_mmu_page_fault ä¸­é—´å°†ä¼šè°ƒç”¨ x86_emulate_instruction æ¥å¤„ç†ï¼Œ
ä¼¼ä¹ç„¶åé€šè¿‡ mmu_notifier ä½¿ç”¨ kvm_mmu_pte_write æ¥æ›´æ–° guest page table

#### page_fault_handle_page_track
direct_page_fault å’Œ FNAME(page_fault) è°ƒç”¨ï¼Œ
ä¼¼ä¹å¦‚æœè¢« trackï¼Œé‚£ä¹ˆè¿™ä¸¤ä¸ªå‡½æ•°ä¼šè¿”å› RET_PF_EMULATE


## track æœºåˆ¶
track å’Œ dirty bitmap å®é™…ä¸Šæ˜¯ä¸¤ä¸ªäº‹æƒ…å§! 

å¯¹äºåŠ ä»¥ç»´æŠ¤çš„:
kvm_slot_page_track_add_page :
kvm_slot_page_track_remove_page :
==> update_gfn_track

- [ ] ä¸¤ä¸ªå‡½æ•°ï¼Œè°ƒç”¨ update,  éƒ½æ˜¯å¯¹äº gfn_track çš„åŠ å‡ 1 è€Œå·²

åˆ†åˆ«è¢« account_shadowed å’Œ unaccount_shadowed è°ƒç”¨

`__kvm_mmu_prepare_zap_page` : è¢«å„ç§ zap page è°ƒç”¨ï¼Œå¹¶ä¸”é…åˆ commit_zap ä½¿ç”¨
=> unaccount_shadowed

kvm_mmu_get_page : 
=> account_shadowed




1. kvm_mmu_page_write

```c
void kvm_mmu_init_vm(struct kvm *kvm)
{
	struct kvm_page_track_notifier_node *node = &kvm->arch.mmu_sp_tracker;

	node->track_write = kvm_mmu_pte_write;
	node->track_flush_slot = kvm_mmu_invalidate_zap_pages_in_memslot;
	kvm_page_track_register_notifier(kvm, node);
}
```
kvm_mmu_get_page: å½“ä¸æ˜¯ direct æ¨¡å¼ï¼Œé‚£ä¹ˆéœ€è¦å¯¹äº kvm_mmu_alloc_page çš„ page è¿›è¡Œ account_shadowed
=> account_shadowed : 
=> kvm_slot_page_track_add_page

**æ‰€ä»¥ï¼Œä¿æŠ¤çš„æ˜¯ shadow page table ?**

```c
static void account_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
{
	struct kvm_memslots *slots;
	struct kvm_memory_slot *slot;
	gfn_t gfn;

	kvm->arch.indirect_shadow_pages++;
	gfn = sp->gfn;
	slots = kvm_memslots_for_spte_role(kvm, sp->role);
	slot = __gfn_to_memslot(slots, gfn);

	/* the non-leaf shadow pages are keeping readonly. */
	if (sp->role.level > PG_LEVEL_4K)
		return kvm_slot_page_track_add_page(kvm, slot, gfn,
						    KVM_PAGE_TRACK_WRITE);

	kvm_mmu_gfn_disallow_lpage(slot, gfn);
}
```
- [ ] ä¸ºä»€ä¹ˆä¸ä¿æŠ¤ leaf shadow page ?

> TOBECON

## track mode

> - dirty tracking:
>    report writes to guest memory to enable live migration
>    and framebuffer-based displays

åŸæ¥ tracing æ˜¯ dirty çš„



```diff
 KVM: page track: add the framework of guest page tracking

 The array, gfn_track[mode][gfn], is introduced in memory slot for every
 guest page, this is the tracking count for the gust page on different
 modes. If the page is tracked then the count is increased, the page is
 not tracked after the count reaches zero

 We use 'unsigned short' as the tracking count which should be enough as
 shadow page table only can use 2^14 (2^3 for level, 2^1 for cr4_pae, 2^2
 for quadrant, 2^3 for access, 2^1 for nxe, 2^1 for cr0_wp, 2^1 for
 smep_andnot_wp, 2^1 for smap_andnot_wp, and 2^1 for smm) at most, there
 is enough room for other trackers

 Two callbacks, kvm_page_track_create_memslot() and
 kvm_page_track_free_memslot() are implemented in this patch, they are
 internally used to initialize and reclaim the memory of the array

 Currently, only write track mode is supported
```

#### gfn_to_memslot_dirty_bitmap
`slot->dirty_bitmap` éƒ½åœ¨ kvm_main ä¸Šé¢è®¿é—®

pte_prefetch_gfn_to_pfn


- [ ] dirty æŒ‡çš„æ˜¯ è° ç›¸å¯¹äº è° æ˜¯ dirty çš„

```c
/**
 * kvm_vm_ioctl_get_dirty_log - get and clear the log of dirty pages in a slot
 * @kvm: kvm instance
 * @log: slot id and address to which we copy the log
 *
 * Steps 1-4 below provide general overview of dirty page logging. See
 * kvm_get_dirty_log_protect() function description for additional details.
 *
 * We call kvm_get_dirty_log_protect() to handle steps 1-3, upon return we
 * always flush the TLB (step 4) even if previous step failed  and the dirty
 * bitmap may be corrupt. Regardless of previous outcome the KVM logging API
 * does not preclude user space subsequent dirty log read. Flushing TLB ensures
 * writes will be marked dirty for next log read.
 *
 *   1. Take a snapshot of the bit and clear it if needed.
 *   2. Write protect the corresponding page.
 *   3. Copy the snapshot to the userspace.
 *   4. Flush TLB's if needed.
 */
static int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm,
				      struct kvm_dirty_log *log)
{
	int r;

	mutex_lock(&kvm->slots_lock);

	r = kvm_get_dirty_log_protect(kvm, log);

	mutex_unlock(&kvm->slots_lock);
	return r;
}
```

https://terenceli.github.io/%E6%8A%80%E6%9C%AF/2018/08/11/dirty-pages-tracking-in-migration

> So here for every gfn, we remove the write access. After return from this ioctl, the guestâ€™s RAM has been marked no write access, every write to this will exit to KVM make the page dirty. This means â€˜start the dirty logâ€™.


- [ ] kvm_mmu_slot_apply_flags : å®é™…ä½œç”¨æ˜¯ dirty log

## kvm_sync_page
kvm_sync_pages : å¯¹äº gfn (å…¶å®æ˜¯ gva å…³è”çš„ vcpu) å…¨éƒ¨æ›´æ–°, é€šè¿‡è°ƒç”¨ kvm_sync_page

kvm_mmu_sync_roots : ä»æ ¹èŠ‚ç‚¹æ›´æ–°æ›´æ–° => (mmu_sync_children : å°†æ•´ä¸ª children è¿›è¡Œ sync)

æœ€ç»ˆè°ƒç”¨ sync_page å‡½æ•°æŒ‡é’ˆç»´æŒç”Ÿæ´»







## mmio
- [ ] å¯¹äº host è€Œè¨€ï¼Œå­˜åœ¨ pcie åˆ†é… mmio çš„åœ°å€ç©ºé—´ï¼Œåœ¨è™šæ‹Ÿæœºä¸­é—´ï¼Œè¿™ä¸€ä¸ªæ˜¯å¦‚ä½•åˆ†é…çš„ MMIO ç©ºé—´çš„

```c
static bool is_mmio_spte(u64 spte)
{
	return (spte & SPTE_SPECIAL_MASK) == SPTE_MMIO_MASK;
}
```

- generation åªæ˜¯ä¸ºäº† MMIO è€Œå¤„ç†çš„


> - if the RSV bit of the error code is set, the page fault is caused by guest
>  accessing MMIO and cached MMIO information is available.
>
>  - walk shadow page table
>  - check for valid generation number in the spte (see "Fast invalidation of
>    MMIO sptes" below)
>  - cache the information to `vcpu->arch.mmio_gva`, `vcpu->arch.mmio_access` and
>    `vcpu->arch.mmio_gfn`, and call the emulator


## mmio generation
ğŸ‘‡è®°å½• mmu.rst çš„å†…å®¹:
è™½ç„¶çš„ç¡®è§£é‡Šäº† mmio ä½¿ç”¨ generation çš„åŸå› ï¼Œä½†æ˜¯ä¸‹é¢çš„é—®é¢˜å€¼å¾—ç†è§£:
- [ ] As mentioned in "Reaction to events" above, kvm will cache MMIO information in leaf sptes. 
  - [ ] å¦‚æœä¸ cache, è¿™äº›æ•°æ®æ”¾åœ¨é‚£é‡Œ

- [ ] When a new memslot is added or an existing memslot is changed, this information may become stale and needs to be invalidated.
  - [ ] ä¸ºä»€ä¹ˆ memslot å¢åŠ ï¼Œå¯¼è‡´æ•°æ®å¤±æ•ˆ

Unfortunately, a single memory access might access kvm_memslots(kvm) multiple
times, the last one happening when the generation number is retrieved and
stored into the MMIO spte.  Thus, the MMIO spte might be created based on
out-of-date information, but with an up-to-date generation number.

- [ ] To avoid this, the generation number is incremented again after synchronize_srcu
returns;

- [ ] æ‰¾åˆ°è®¿é—® pte æ¥æ¯”è¾ƒ generation, å‘ç° out of dateï¼Œç„¶å slow path çš„ä»£ç 

## TODO : shadow flood


## kvm_main
kvm_is_zone_device_pfn

- [ ] zone device


#### vcpu_load
- [ ] check ä¸€ä¸‹ä½¿ç”¨çš„ä½ç½®
- [ ] preempt_notifier_register : ç¥å¥‡çš„ notifier æœºåˆ¶
- [ ] å’Œ vcpu_put çš„ç»“åˆåˆ†æä¸€ä¸‹

```c
/*
 * Switches to specified vcpu, until a matching vcpu_put()
 */
void vcpu_load(struct kvm_vcpu *vcpu)
{
	int cpu = get_cpu();

	__this_cpu_write(kvm_running_vcpu, vcpu);
	preempt_notifier_register(&vcpu->preempt_notifier);
	kvm_arch_vcpu_load(vcpu, cpu);
	put_cpu();
}
```

#### kvm_vm_ioctl_set_memory_region

#### kvm_vcpu_unmap

#### kvm_read_guest
- [ ] ä¸ºä»€ä¹ˆè¦å¤„ç† guest page æœºåˆ¶

#### kvm_vcpu_fault
> é…åˆ vcpu ioctl 
```c
static int create_vcpu_fd(struct kvm_vcpu *vcpu)
{
	char name[8 + 1 + ITOA_MAX_LEN + 1];

	snprintf(name, sizeof(name), "kvm-vcpu:%d", vcpu->vcpu_id);
	return anon_inode_getfd(name, &kvm_vcpu_fops, vcpu, O_RDWR | O_CLOEXEC);
}
```

#### kvm device ioctl
> TODO

#### kvm io bus write

kvm_io_bus_write => `__kvm_io_bus_write`

```c
struct kvm_io_bus {
	int dev_count;
	int ioeventfd_count;
	struct kvm_io_range range[];
};
```
KVM: Adds support for in-kernel mmio handlers


## unsorted resource
- Extended page-table mechanism (EPT) used to support the virtualization of physical memory.
- **Translates the guest-physical addresses used in VMX non-root operation.**
- Guest-physical addresses are translated by traversing a set of EPT paging structures to produce physical addresses that are used to access memory.


> 1. å¯¹äº page table çš„ç¿»è¯‘ : è®©ç¡¬ä»¶å®Œæˆå…¶ä¸­çš„æ’å…¥å·¥ä½œï¼Œè¿™æ ·å°±ä¸ä½¿ç”¨ shadow table
> 2. ä½¿ç”¨ TLB è¿›è¡Œç¿»è¯‘


> TLB è¢«åˆ’åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼Œ`VA->PA` å’Œ `PA-VA`


hspt çš„æƒ³æ³• : å°†å†…æ ¸ä¸­é—´æ·»åŠ ä¸€ä¸ª mmap çš„ç©ºé—´ï¼Œæ¯ä¸€ä¸ª process åœ¨ä¸€ä¸ªè™šæ‹Ÿåœ°å€ç©ºé—´ä¸­é—´ï¼Œ
è¿™ä¸ªè™šæ‹Ÿåœ°å€ç©ºé—´ç›´æ¥æ˜ å°„åˆ° host çš„ä¸€ä¸ªè¿ç»­ç©ºé—´ä¸­é—´ï¼Œé‚£ä¹ˆè®¿é—®å°±ç›¸å½“äºç›´æ¥è®¿é—®äº†.

- Simplified VMM design. éœ€è¦å¤„ç† shadow page table å’Œä¸¤çº§ç¿»è¯‘çš„åŒæ­¥é—®é¢˜
- Guest page table modifications need not be trapped, hence VM exits reduced. åŒæ­¥
- Reduced memory footprint compared to shadow page table algorithms. shadow table ä¼šå ç”¨ç©ºé—´

TLB miss is very costly since guest-physical address to machine address needs an extra EPT walk for each stage of guest-virtual address translation.


## kvm_make_all_cpus_request

## hypercall
https://stackoverflow.com/questions/33590843/implementing-a-custom-hypercall-in-kvm

x86.c: kvm_emulate_hypercall

```c
/* For KVM hypercalls, a three-byte sequence of either the vmcall or the vmmcall
 * instruction.  The hypervisor may replace it with something else but only the
 * instructions are guaranteed to be supported.
 *
 * Up to four arguments may be passed in rbx, rcx, rdx, and rsi respectively.
 * The hypercall number should be placed in rax and the return value will be
 * placed in rax.  No other registers will be clobbered unless explicitly
 * noted by the particular hypercall.
 */

static inline long kvm_hypercall0(unsigned int nr)
{
	long ret;
	asm volatile(KVM_HYPERCALL
		     : "=a"(ret)
		     : "a"(nr)
		     : "memory");
	return ret;
}
```
host å‘é€ hypercall çš„ä¹‹åï¼Œé€ æˆä» host ä¸­é—´é€€å‡ºï¼Œç„¶å æœ€åè°ƒç”¨åˆ° kvm_emulate_hypercall, å®é™…ä¸Šæ”¯æŒçš„æ“ä½œå¾ˆå°‘

```c
int kvm_emulate_hypercall(struct kvm_vcpu *vcpu)
{
	unsigned long nr, a0, a1, a2, a3, ret;
	int op_64_bit;

  // TODO hyperv å¦ä¸€ç§è™šæ‹ŸåŒ–æ–¹æ¡ˆ ?
  // ä¸€ç§ç¡¬ä»¶æ”¯æŒ ?
	if (kvm_hv_hypercall_enabled(vcpu->kvm))
		return kvm_hv_hypercall(vcpu);

```

## resources
- https://github.com/dpw/kvm-hello-world : a good resource to understand how real, protect, long mode in intel
- https://github.com/david942j/kvm-kernel-example

- [Watch this organization](https://github.com/rust-vmm/kvm-bindings)
>  It provides a set of virtualization components that any project can use to quickly develop virtualization solutions while focusing on the key differentiators of their product rather than re-implementing common components like KVM wrappers, virtio devices and other VMM libraries.


- https://github.com/canonical/multipass
  - write with cpp
  - include many cpp 

#### hypervisor-from-scratch-part-4
https://rayanfam.com/topics/hypervisor-from-scratch-part-4/
> TODO intel æ‰‹å†Œ Chapter 28 â€“ (VMX SUPPORT FOR ADDRESS TRANSLATION)

> According to a VMware evaluation paper: â€œEPT provides performance gains of up to 48% for MMU-intensive benchmarks and up to 600% for MMU-intensive microbenchmarksâ€.

- [ ] æ˜¯ shadow table éœ€è¦ä½¿ç”¨ Complicated reverse map çš„å— ?

> **EPT mechanism that treats your Guest Physical Address like a virtual address and the EPTP is the CR3.**

- [ ] cr3 ä¸­é—´å­˜æ”¾ eptp çš„åœ°å€ï¼Œæ‰¾åˆ°å¯¹åº”çš„ä»£ç  ?

> Note that PAE stands for **Physical Address Extension** which is a memory management feature for the x86 architecture that extends the address space and PSE stands for **Page Size Extension** that refers to a feature of x86 processors that allows for pages larger than the traditional 4 KiB size.
