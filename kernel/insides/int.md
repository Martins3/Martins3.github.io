# introduction
idt_data 和 gate_desc 的关系是什么 ? 应该是保存的信息是相同的，但是idt_data 的内存布局更好，容易赋值和操作而已。

The TSS is used for stack switching during the execution of an interrupt or exception handler in the Linux kernel

ist 和 tss 实现新的stack 切换 和 irq_stack_union等初始化关系是什么？ 对于某些 interrupt 通过ist和TSS机制，ist是TSS 的index， TSS 中间存储具体的stack位置，当出现某些int, 自动跳转到对应的stack 中间，采用percpu 初始化这些stack 只是由于每一个cpu 都可以中断了

We already know that x86_64 has a feature called Interrupt Stack Table or IST and this feature provides the ability to switch
to a new stack for events non-maskable interrupt, double fault etc. 
There can be up to seven IST entries per-cpu. Some of them are:
```c
/*
 * the exceptions which use interrupt stacks. they are setup after
 * cpu_init() when the tss has been initialized.
 */
static const __initconst struct idt_data ist_idts[] = {
	istg(x86_trap_db,	debug,		debug_stack),
	istg(x86_trap_nmi,	nmi,		nmi_stack),
	istg(x86_trap_df,	double_fault,	doublefault_stack),
#ifdef config_x86_mce
	istg(x86_trap_mc,	&machine_check,	mce_stack),
#endif
};
```

interrupt 的时候，如何处理权限变化 ?

# Start to dive into interrupt and exceptions handling in the Linux kernel

head_64.S
```
movl    $MSR_GS_BASE,%ecx
movl    initial_gs(%rip),%eax
movl    initial_gs+4(%rip),%edx // @todo 上面两部操作最后将 initial_gs 的地址放到gs中间，如果gs 是per_cpu 的基址寄存器，那么如何保证每一个CPU中间的gs数值。
wrmsr
```
initial_gs 同样定义在head_64.S 中间，

https://en.wikipedia.org/wiki/X86_memory_segmentation
https://www.kernel.org/doc/Documentation/this_cpu_ops.txt 含有关于gs的描述


gs 初始化，设置idt, sti & cli

@todo 初始化idt 的时候，为什么在开始的静态初始化啊! 而是在最后调用函数初始化idt
@todo per_cpu 的 sti 和 cli 的有什么不同，lockdep 和开关中断有什么关联


# Exception Handling
介绍 debug point 和 break point

```
testl $3,CS(%rsp) // @todo 实在是没有搞懂amd64 的寻址方式啊 !
jnz userspace
```

介绍了idtentry 宏的实现过程 // @todo 中间关于原子性的部分没有看懂


idtentry 应该是通用的入口吧,曾经一直疑惑的syscall 如何切换地址空间的问题就在此处解决了。
> 解决的问题包括: syscall 在内核中间的stack 从哪里来，如何

当整个syscall 调用完成的时候，系统返回，

> paranoid 


# Initialization of non-early interrupt gates
> 为什么初始化中断总是需要一个一个完成，现在又跑过来初始化page fault

```
trace_idtentry page_fault do_page_fault has_error_code=1
```
> 难怪自己没有办法找到page_fault，原来又是定义在asm 中间了

简要的描述了一下page fault 底层的内容，形成互补，nice !

再次分析 trap_init，其中装载idt 在 idt_setup_traps(); 中间，和文档描述的不同， 而cpu_init();中间讲解如何将重新装填的idt 加载到CPU 中间，并且再次分析了TSS

> 和之前初始化的函数，调用的时间顺序是什么？

# Implementation of some exception handlers
We saw only setting of these interrupt gates in the previous part and in the current part we will see implementation of the exception handlers for these gates

In another words the idtentry macro allocates place for the registers (pt_regs structure) on the stack, pushes dummy error code for the stack consistency if an interrupt/exception has no error code, checks the segment selector in the cs segment register and switches depends on the previous state(userspace or kernelspace). 
> 良好的总结

The context tracking in the Linux kernel subsystem which provide kernel boundaries probes to keep track of the transitions between level contexts with two basic initial contexts: user or kernel
> 这是 exception_enter 处理的事情，但是 do_page_fault 中间 可以直接 disable 掉这一个功能


> @question 第一种 interrupt & exception
总结一下代码流程图:
1. entry_64.S 中间通过 idtentry 定义 idt 需要的函数入口
2. traps.c 中间通过 DO_ERROR 的 micro 定义了一系列的类似于 do_overflow 的错误
3. DO_ERROR 宏定义的函数都是调用函数 do_error_trap 
4. do_error_trap 经过现在已经取消，而且没有怎么看懂的 exception_enter 和 exception_exit 函数包围之后，然后调用 do_trap
5. do_trap 通过 do_trap_no_signal 实现检查错误是否来自于内核，由于这些各种同步错误不应该来自于内核中间，所以会调用 fixup_exeception 来抢救一下
6. 最后通过 force_sig_info 将 signal 传递给被 interrupted process


double fault 机制:
1. 入口函数 do_double_fault
2. espfix feature
3. ist_enter
4. die
> ist 应该没有讲过吧! https://www.kernel.org/doc/Documentation/x86/kernel-stacks
> double fault 一定会导致 die 吗 ?

后面还分析了:General protection fault exception handler 和 Device not available exception handler
应该在 entry_64.S 中间定义了 13 个项目，但是 DO_ERROR 一共处理了其中的 8个，所以其他由于某些细节原因，
不能共同使用 DO_ERROR 机制，所以采用单独分析。

> 这些13个idt entry 有什么共同的特点 ?，其他的idt 如何注册上去的


https://wiki.osdev.org/Exceptions
> 首先阅读这一个文档，描述所有的x86 的exception 的内容

> context tracking 的功能是直接disable 的
> 神奇的 notifier 机制

# Non-maskable interrupt handler
https://wiki.osdev.org/Interrupt_Vector_Table
> 虽然前面32 位置是给CPU 预留的，但是实际上只有13个被使用，这13个全部被作者分析了



> 对于上一部分继续陈述的效果

nmi 不使用通用的 idtentry
> @todo 详细的描述了 entry_64 nmi的汇编实现过程

> 不停出现的 exception_enter 和 exception_exit 似乎都是不存在的



# Introduction to external interrupts
This is the seventh part of the Interrupts and Interrupt Handling in the Linux kernel chapter and in the previous part we have finished with the exceptions which are generated by the processor
> 前面分析的内容限于 trap_init ， 同样在 start_kernel 中间, 但是两个函数之间的距离非常的遥远，此时分析 early_irq_init 

The ISR or Interrupt Handler Routine can be found in Interrupt Vector table that is located at fixed address in the memory。

> 中断三张表: idt, interrupt vector table , syscall table
> - [ ] 真的存在 interrupt vector table 吗 ?


As we saw in the previous parts, most exceptions are handled simply by the sending a Unix signal to the interrupted process
> - [ ]  exception 的确象是将 signal 发送到 process 就可以了, trace 一下代码

Generally, a handler of an I/O interrupt must be flexible enough to service several devices at the same time.

> irq_desc 其中的内容长的一B
> sparse irq  ???
> cpumask 实现部分irq 发配到特定的CPU 中间处理

调用层次:
1. start_kernel
2. early_irq_init
    1. init_irq_default_affinity
    2. arch_probe_nr_irqs // 探测一共含有多少个 irq
    3. desc 默认初始化
    4. arch

> early_irq_init 中间分析的 NR_IRQS 是编译期间就确定的macro，但是实际上for loop 使用的 init_cnt ，并且将 irq_desc 插入到radix tree 中间

> @todo 关于 spare irq 的分析

`NR_IRQS` depends on the amount of the processors and amount of the interrupt vectors:
> 认知颠覆了

> - [ ] 讲解的内容都是irq 的默认初始化，而且初始化的数目惊人，此外，并不知道如何从idt 中间进入irq 中间，而且irq_desc 的结构如此复杂，并不是很清楚为什么要这么做，
> - [ ] 如何和具体设备的关联现在也不知道 ?

# Non-early initialization of the IRQs
Right after the call of the `early_irq_init` function in the `init/main.c` we can see the call of the `init_IRQ` function.


1. 分析init_IRQ one line (complex)

for every cpu, we define a `vector_irq` which is 256
> 256 , last part , count

```c
#define per_cpu(var, cpu)	(*per_cpu_ptr(&(var), cpu))


struct irq_desc *irq_to_desc(unsigned int irq) // radix tree ?
{
	return radix_tree_lookup(&irq_desc_tree, irq);
}
EXPORT_SYMBOL(irq_to_desc);
```
> irq_to_desc 书中描述自相矛盾，此处初始化ISA总线的内容，此外 vector_irq 的类型从原来的数值变成了现在的 irq_desc * , 并且添加 radix_tree_lookup 的操作
> @todo 那么从 0x20 ~ 0x30 之间的初始化工作谁来处理 ?

2. `native_init_IRQ` 初始化APIC 和 ISA 中断
3. `init_ISA_irqs` (oops IAS, where is the fucking PCIe, I think there should be complex PCIe driver to connect the irqs)
4. `init_bsp_APIC();`
5. `clear_local_APIC()`
6. 然后通过 APIC_VECTOR_MASK 将 bootstrap cpu 的 apic 打开的
7. init_8259A

Now we can return to the native_init_IRQ function, after the init_ISA_irqs function finished its work. The next step is the call of the `apic_intr_init` function that allocates special interrupt gates 
> 从 native_init_IRQ 到 init_ISA_irqs 

```c
void __init native_init_IRQ(void)
{
	/* Execute any quirks before the call gates are initialised: */
	x86_init.irqs.pre_vector_init(); // 实际调用位置 init_ISA_irqs  @todo 使用如此之多的封装到底是为了什么?

	idt_setup_apic_and_irq_gates(); // 这应该就是之后开始分析的位置  
	lapic_assign_system_vectors();

	if (!acpi_ioapic && !of_ioapic && nr_legacy_irqs())
		setup_irq(2, &irq2);

	irq_ctx_init(smp_processor_id());
}
```

As we can see, first of all it expands to the call of the alloc_system_vector function that checks the given vector number in the `used_vectors` bitmap 

> 作者顺便分析了 test_bit 之类的操作

The next step after the `apic_intr_init` function will finish its work is the setting interrupt gates from the `FIRST_EXTERNAL_VECTOR` or 0x20 to the 0x256:

> 然后继续分析 idt_setup_apic_and_irq_gates

In the end of the `native_init_IRQ` function we can see the following check:

> 最后回到 native_init_IRQ 中间, 开始分析 setup_irq 函数， 总结就是逐行的分析 init_IRQ 函数

> 感觉 setup_irq 才是最终注册的关键
> apic,  irq table 到底是什么回事 ?

because probably APIC handles interrupts on the our machine.

通过  /proc/interrupts 和 /proc/irq/2 的内容让觉得 interrupt 和 irq 关系是什么产生怀疑

> APIC作为一个外部设备，使用驱动来运行，还是说x86中间必定含有APIC的，所以init_IRQ 中间会注册这一个，应该说是，内核中间必定包含，含有 apic_read 之类的函数

In this part we already saw non-early interrupts initialization in the `init_IRQ` function. 
We saw initialization of the `vector_irq` per-cpu array which is store vector numbers of the interrupts 
and will be used during interrupt handling and initialization of other stuff which is related to the external hardware interrupts.

The role of the PIC will be to decide whether the CPU should be immediately notified of that IRQ or 
not and to translate the `IRQ number` into an `interrupt vector` (i.e. a number between 0 and 255) for the CPU's table.
> - 惊不惊喜，刺不刺激，我们拥有两套系统数值系统
> - 关键是，键盘的IRQ 是如何传递到达AIPC 的

> 先看个 linux doc
> https://www.kernel.org/doc/html/v4.17/core-api/genericirq.html

> do_IRQ

> entry_64.S 中:
> interrupt_entry => do_IRQ

> interrupt讲授细节过多，找点overview 的资料吧!
> irq_desc 的作用是什么 ?
> riq_chip 用于封装什么东西 ?

# Introduction to deferred interrupts (Softirq, Tasklets and Workqueues)
The `spawn_ksoftirqd` function starts this these threads.

```c
void open_softirq(int nr, void (*action)(struct softirq_action *))
{
	softirq_vec[nr].action = action;
}
```

First of all let's look on the `softirq_vec` array:

So, after this we can understand that the open_softirq function fills the `softirq_vec` array with the given `softirq_action`

The registered deferred interrupt (with the call of the `open_softirq` function) for it to be queued for execution, 
it should be activated by the call of the `raise_softirq` function


```c
/*
 * This function must run with irqs disabled!
 */
inline void raise_softirq_irqoff(unsigned int nr)
{
	__raise_softirq_irqoff(nr); // 只是简单的设置一个percpu 的变量的nr 位置

	/*
	 * If we're in an interrupt or softirq, we're done
	 * (this also catches softirq-disabled code). We will
	 * actually run the softirq once we return from
	 * the irq or softirq.
	 *
	 * Otherwise we wake up ksoftirqd to make sure we
	 * schedule the softirq soon.
	 */
	if (!in_interrupt()) // 不在interrupt 那么启动内核线程, 这一个函数的实现也是非常有意思
		wakeup_softirqd();
}

void raise_softirq(unsigned int nr)
{
	unsigned long flags;

	local_irq_save(flags); // @todo 如果不disable 如何 ?
	raise_softirq_irqoff(nr);
	local_irq_restore(flags);
}

void __raise_softirq_irqoff(unsigned int nr)
{
	trace_softirq_raise(nr);
	or_softirq_pending(1UL << nr);
}


/*
 * we cannot loop indefinitely here to avoid userspace starvation,
 * but we also don't want to introduce a worst case 1/HZ latency
 * to the pending events, so lets the scheduler to balance
 * the softirq load for us.
 */
static void wakeup_softirqd(void)
{
	/* Interrupts are disabled: no need to stop preemption */
	struct task_struct *tsk = __this_cpu_read(ksoftirqd);

	if (tsk && tsk->state != TASK_RUNNING)
		wake_up_process(tsk);
}
```

Each ksoftirqd kernel thread runs the `run_ksoftirqd` function that checks existence of deferred interrupts and calls the `__do_softirq` function
depending on the result of the check.

```c
static struct smp_hotplug_thread softirq_threads = {
	.store			= &ksoftirqd,
	.thread_should_run	= ksoftirqd_should_run,
	.thread_fn		= run_ksoftirqd,
	.thread_comm		= "ksoftirqd/%u",
};

static __init int spawn_ksoftirqd(void)
{
	cpuhp_setup_state_nocalls(CPUHP_SOFTIRQ_DEAD, "softirq:dead", NULL,
				  takeover_tasklets);
	BUG_ON(smpboot_register_percpu_thread(&softirq_threads));

	return 0;
}
early_initcall(spawn_ksoftirqd); // 有意思，很强的
```
> ksoftirqd 和 run_ksoftirqd　的挂钩在什么时候

```c
static void run_ksoftirqd(unsigned int cpu)
{
	local_irq_disable();
	if (local_softirq_pending()) {
		/*
		 * We can safely run softirq on inline stack, as we are not deep
		 * in the task stack here.
		 */
		__do_softirq();
		local_irq_enable();
		cond_resched();
		return;
	}
	local_irq_enable();
}
```
> 书上还讨论 1/HZ 的时间问题，从代码注释看，似乎已经被取消了

To summarize, each softirq goes through the following stages:
1. Registration of a softirq with the `open_softirq` function.
2. Activation of a softirq by marking it as deferred with the `raise_softirq` function.
3. After this, all marked softirqs will be triggered in the next time the Linux kernel schedules a round of executions of deferrable functions.
4. And execution of the deferred functions that have the same type.
> 似乎不错，但是执行的函数是什么东西，如果出现多次 register softirq 会不会多次执行，如果不会，为什么可以不用多次执行
> 谁来调用 open_softirq 这一个函数


As I already wrote above the tasklets are built on top of the softirq concept and generally on top of two softirqs:
1. TASKLET_SOFTIRQ;
2. HI_SOFTIRQ.

```c
struct tasklet_head {
	struct tasklet_struct *head;
	struct tasklet_struct **tail; // emmmmm, two level pointer
};

static DEFINE_PER_CPU(struct tasklet_head, tasklet_vec);
static DEFINE_PER_CPU(struct tasklet_head, tasklet_hi_vec);


void __init softirq_init(void)
{
	int cpu;

	for_each_possible_cpu(cpu) { // 清空
		per_cpu(tasklet_vec, cpu).tail =
			&per_cpu(tasklet_vec, cpu).head;
		per_cpu(tasklet_hi_vec, cpu).tail =
			&per_cpu(tasklet_hi_vec, cpu).head;
	}

	open_softirq(TASKLET_SOFTIRQ, tasklet_action); // 注册操作
	open_softirq(HI_SOFTIRQ, tasklet_hi_action);
}
```

The Linux kernel provides three following functions to mark a tasklet as ready to run:
```c
// 启动方法
void tasklet_schedule(struct tasklet_struct *t);
void tasklet_hi_schedule(struct tasklet_struct *t);
void tasklet_hi_schedule_first(struct tasklet_struct *t);


void tasklet_init(struct tasklet_struct *t,           // 初始化方法
		  void (*func)(unsigned long), unsigned long data)
{
	t->next = NULL;
	t->state = 0;
	atomic_set(&t->count, 0);
	t->func = func;
	t->data = data;
}
EXPORT_SYMBOL(tasklet_init);
```


```c
static __latent_entropy void tasklet_action(struct softirq_action *a)
{
	tasklet_action_common(a, this_cpu_ptr(&tasklet_vec), TASKLET_SOFTIRQ);
}


static void tasklet_action_common(struct softirq_action *a,
				  struct tasklet_head *tl_head,
				  unsigned int softirq_nr)
{
	struct tasklet_struct *list;

	local_irq_disable();
	list = tl_head->head;
	tl_head->head = NULL;
	tl_head->tail = &tl_head->head;
	local_irq_enable();

	while (list) {
		struct tasklet_struct *t = list;

		list = list->next;

		if (tasklet_trylock(t)) {
			if (!atomic_read(&t->count)) {
				if (!test_and_clear_bit(TASKLET_STATE_SCHED,
							&t->state))
					BUG();
				t->func(t->data); // 循环链表的的开始执行其中的内容
				tasklet_unlock(t);
				continue;
			}
			tasklet_unlock(t);
		}

		local_irq_disable();
		t->next = NULL;
		*tl_head->tail = t;
		tl_head->tail = &t->next;
		__raise_softirq_irqoff(softirq_nr);
		local_irq_enable();
	}
}
```

> 1. 为什么需要softirq 的理由让人怀疑
> 2. 是 hardirq 在使用 soft


Workqueue functions run in the context of a **kernel** process, but tasklet functions run in the software interrupt context
This means that workqueue functions must not be **atomic** as tasklet functions. 
Tasklets always run on the processor from which they were originally submitted. Workqueues work in the same way, but only by default.

In its most basic form, the work queue subsystem is an interface for creating kernel threads to handle work that is **queued** from elsewhere.
> 忽然变得不限于 interrupt 了


```c
/* struct worker is defined in workqueue_internal.h */

struct worker_pool {


struct work_struct {
	atomic_long_t data;
	struct list_head entry;
	work_func_t func;
#ifdef CONFIG_LOCKDEP
	struct lockdep_map lockdep_map;
#endif
};
```
> 所以这连个结构体是做什么 ?


After a work was created with the one of these macros, we need to put it to the workqueue. We can do it with the help of the queue_work or the `queue_delayed_work` functions:

> 接下来逐级对于queue_work 向下分析


# Last part
21285 初始化:

When an uart port will be opened with the call of the `uart_open` function from the `drivers/tty/serial/serial_core.c`,
it will call the `uart_startup` function to start up the serial port.
This function will call the startup function that is part of the `uart_ops` structure. Each uart driver has the definition of this structure, in our case it is:
> 似乎调用关系我们不是很清楚

```c
static const struct uart_ops serial21285_ops = {
	.tx_empty	= serial21285_tx_empty,
	.get_mctrl	= serial21285_get_mctrl,
	.set_mctrl	= serial21285_set_mctrl,
	.stop_tx	= serial21285_stop_tx,
	.start_tx	= serial21285_start_tx,
	.stop_rx	= serial21285_stop_rx,
	.break_ctl	= serial21285_break_ctl,
	.startup	= serial21285_startup, // 分析的关键
	.shutdown	= serial21285_shutdown,
	.set_termios	= serial21285_set_termios,
	.type		= serial21285_type,
	.release_port	= serial21285_release_port,
	.request_port	= serial21285_request_port,
	.config_port	= serial21285_config_port,
	.verify_port	= serial21285_verify_port,
};


static inline int __must_check
request_irq(unsigned int irq, irq_handler_t handler, unsigned long flags,
	    const char *name, void *dev)
{
	return request_threaded_irq(irq, handler, NULL, flags, name, dev);
}


/**
 *	request_threaded_irq - allocate an interrupt line
 *	@irq: Interrupt line to allocate
 *	@handler: Function to be called when the IRQ occurs.
 *		  Primary handler for threaded interrupts
 *		  If NULL and thread_fn != NULL the default
 *		  primary handler is installed
 *	@thread_fn: Function called from the irq handler thread
 *		    If NULL, no irq thread is created
 *	@irqflags: Interrupt type flags
 *	@devname: An ascii name for the claiming device
 *	@dev_id: A cookie passed back to the handler function
 *
 *	This call allocates interrupt resources and enables the
 *	interrupt line and IRQ handling. From the point this
 *	call is made your handler function may be invoked. Since
 *	your handler function must clear any interrupt the board
 *	raises, you must take care both to initialise your hardware
 *	and to set up the interrupt handler in the right order.
 *
 *	If you want to set up a threaded irq handler for your device
 *	then you need to supply @handler and @thread_fn. @handler is
 *	still called in hard interrupt context and has to check
 *	whether the interrupt originates from the device. If yes it
 *	needs to disable the interrupt on the device and return
 *	IRQ_WAKE_THREAD which will wake up the handler thread and run
 *	@thread_fn. This split handler design is necessary to support
 *	shared interrupts.
 *
 *	Dev_id must be globally unique. Normally the address of the
 *	device data structure is used as the cookie. Since the handler
 *	receives this value it makes sense to use it.
 *
 *	If your interrupt is shared you must pass a non NULL dev_id
 *	as this is required when freeing the interrupt.
 *
 *	Flags:
 *
 *	IRQF_SHARED		Interrupt is shared
 *	IRQF_TRIGGER_*		Specify active edge(s) or level
 *
 */
int request_threaded_irq(unsigned int irq, irq_handler_t handler,
			 irq_handler_t thread_fn, unsigned long irqflags,
			 const char *devname, void *dev_id)
{
	struct irqaction *action;
	struct irq_desc *desc;
	int retval;

	if (irq == IRQ_NOTCONNECTED)
		return -ENOTCONN;

	/*
	 * Sanity-check: shared interrupts must pass in a real dev-ID,
	 * otherwise we'll have trouble later trying to figure out
	 * which interrupt is which (messes up the interrupt freeing
	 * logic etc).
	 *
	 * Also IRQF_COND_SUSPEND only makes sense for shared interrupts and
	 * it cannot be set along with IRQF_NO_SUSPEND.
	 */
	if (((irqflags & IRQF_SHARED) && !dev_id) ||
	    (!(irqflags & IRQF_SHARED) && (irqflags & IRQF_COND_SUSPEND)) ||
	    ((irqflags & IRQF_NO_SUSPEND) && (irqflags & IRQF_COND_SUSPEND)))
		return -EINVAL;

	desc = irq_to_desc(irq);
	if (!desc)
		return -EINVAL;

	if (!irq_settings_can_request(desc) ||
	    WARN_ON(irq_settings_is_per_cpu_devid(desc)))
		return -EINVAL;

	if (!handler) {
		if (!thread_fn)
			return -EINVAL;
		handler = irq_default_primary_handler;
	}

	action = kzalloc(sizeof(struct irqaction), GFP_KERNEL);
	if (!action)
		return -ENOMEM;

	action->handler = handler;
	action->thread_fn = thread_fn;
	action->flags = irqflags;
	action->name = devname;
	action->dev_id = dev_id;

	retval = irq_chip_pm_get(&desc->irq_data);
	if (retval < 0) {
		kfree(action);
		return retval;
	}

	retval = __setup_irq(irq, desc, action);

	if (retval) {
		irq_chip_pm_put(&desc->irq_data);
		kfree(action->secondary);
		kfree(action);
	}

#ifdef CONFIG_DEBUG_SHIRQ_FIXME
	if (!retval && (irqflags & IRQF_SHARED)) {
		/*
		 * It's a shared IRQ -- the driver ought to be prepared for it
		 * to happen immediately, so let's make sure....
		 * We disable the irq to make sure that a 'real' IRQ doesn't
		 * run in parallel with our fake.
		 */
		unsigned long flags;

		disable_irq(irq);
		local_irq_save(flags);

		handler(irq, dev_id);

		local_irq_restore(flags);
		enable_irq(irq);
	}
#endif
	return retval;
}
EXPORT_SYMBOL(request_threaded_irq);


static int serial21285_startup(struct uart_port *port)
{
	int ret;

	tx_enabled(port) = 1;
	rx_enabled(port) = 1;

	ret = request_irq(IRQ_CONRX, serial21285_rx_chars, 0, // @todo IRQ_CONRX 的数值定义莫名奇妙啊!
			  serial21285_name, port);
	if (ret == 0) {
		ret = request_irq(IRQ_CONTX, serial21285_tx_chars, 0,
				  serial21285_name, port);
		if (ret)
			free_irq(IRQ_CONRX, port);
	}

	return ret;
}
```
This function registers an interrupt handler and enables a given interrupt line.

> @todo the later part is interesting, we will fix it !
> what we expected is :
> 1. will we affect  idt vector ? or only interact with APIC
> 2. how to register driver irq handler ?

# 附录
https://wiki.osdev.org/PIC
> 如果不了解 PIC 和　APIC 的功能，不可能理解其中的代码的

什么叫做 interrupt line 和 irq line 是一个东西吗 ?
https://reverseengineering.stackexchange.com/questions/16975/whats-the-difference-between-an-interrupt-line-and-the-interrupt-number

