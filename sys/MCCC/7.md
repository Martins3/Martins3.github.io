# Snooping Coherence Protocols
Snooping protocols
offer many attractive features, including **low-latency** coherence transactions and a **conceptually simpler** design than the alternative, directory protocols

## 7.1 INTRODuCTION TO SNOOPINg
By requiring that all requests to a given block arrive in order, a snooping system enables the distributed coherence
controllers to correctly update the finite state machines that collectively represent a cache block’s state.
> 顺序，应该是只有保证一个core中间的消息到达其他的所有core的消息顺序是相同的，而不用是
> 接受消息顺序和时间发生的顺序相同
> 每一个core接受顺序相同


The ordered broadcast ensures that every coherence controller observes the
same series of coherence requests in the same order, i.e., that there is a total order of coherence
requests.
> 很显然，这一个上面的想法是错误的:
> 所有core接受相同的信号，并且接受的顺序相同
> 但是我觉得要求过高，除非使用单总线连接所有core, 不可以使用互联网络
> 而且仅仅保证每一个的到达顺序就可以了


> Table7.1和Table7.2中间例子完全无法理解
> 1. 中间对于状态表示，为什么开始的时候都是I(valide), 从哪一个地方开始规定的memory中间的也是I
> 2. 这一个表是用来说明per

Traditional snooping protocols create a total order of coherence requests across all blocks, even
though coherence requires only a per-block order of requests.
> 很显然，这一个要求也可以去除
> 但是Table7.3指出了问题

**如何理解Table7.3**
> 2. 为什么Cache中间的状态S M都是需要包含数值的，但是唯独LLC/Memory唯独M状态不需要包含数值
> 4. 为什么是首先Core C1和LLC/Memory 先接受到GetS的消息
>
> 没有违背了store -> load 的关系，而是违背了store -> store的关系，因为Core C1写入的数值A B先后为1, 但是Core C2首先B = 1 A = 0的情况
> 和6.4.2中间含有描述不一致的地方，但都是大致的含义是　接受到消息/转化的状态

**如何理解Table7.3**
> However, like the proverbial “tree in the forest,”
this violation does not cause a problem because it is not observed
> 所以想要表达什么，是幸运，还是因为使用invalidate A，不矛盾吗?

> Traditional snooping protocols use the total order of coherence requests to determine when, in a
logical time based on snoop order, a particular request has been observed
> 电路上还需要ack !!!





Requiring that broadcast coherence requests be observed in a **total order** has important implications for the interconnection network used to implement traditional snooping protocols.
> 首先，total order到底定义是什么，为什么需要这样定义，是否可以优化放松等
> 暂且任务就是all block same order

Because many coherence controllers may simultaneously attempt to issue coherence requests,
the interconnection network must serialize these requests into some total order.
However the network determines this order, this mechanism becomes known as the **protocol’s serialization (ordering) point**.

In the general case, a coherence controller issues a coherence request, the network orders that request
at the serialization point and broadcasts it to all controllers, and *the issuing controller learns where
its request has been ordered by snooping the stream of requests it receives from the controller.*
> 一个正在发射请求的控制器通过侦测从其他的控制器发送过来的请求流可以知道自己的请求在哪里被order
> 首先，这是如何办到的
> 其次，为什么一个控制器需要知道自己的请求在哪里被order

As a concrete and simple example, consider a system which uses a bus to broadcast coherence requests.
Coherence controllers must use arbitration logic to ensure that only a single request is issued on the
bus at once. This arbitration logic acts as the serialization point because it effectively determines
the order in which requests appear on the bus.

A subtle but important point is that a coherence
request is ordered the instant the arbitration logic serializes it, but *a controller may only be able to
determine this order by snooping the bus to observe which other requests appear before and after
its own request.* Thus, coherence controllers may observe the total request order several cycles after
the serialization point determines it.
> 似乎和上面那句对应
> 但是，关键的问题在于，就算延迟几个周期那又如何?

Thus far, we have discussed only coherence requests, but not the responses to these requests.
**The reason for this seeming oversight is that the key aspects of snooping protocols revolve around
the requests.**

There are few constraints on response messages. They can travel on a separate interconnection network that does not need to support broadcast nor have any ordering requirements.
Because response messages carry data and are thus much longer than requests, there are significant
benefits to being able to send them on a simpler, lower-cost network. **Notably, response messages
do not affect the serialization of coherence transactions.**

Logically, a coherence transaction—which
consists of a broadcast request and a unicast response—occurs when the request is ordered, regardless of when the response arrives at the requestor.
The time interval between when the request appears
on the bus and when the response arrives at the requestor does affect the implementation of the
protocol (e.g., during this gap, are other controllers allowed to request this block? If so, how does
the requestor respond?), but it does not affect the serialization of the transaction.

## 7.2 BASELINE SNOOPINg PROTOCOL
### 7.2.1 High-Level Protocol Specification
*A block is owned by the LLC/memory unless the block is in a cache in state M*
> 需要阅读更多来分析

we specify the state of a block at memory using a cache-centric notation
(e.g., a memory state of M denotes that there exists a cache with the block in state M)
> 说实话，这一个规定很不自然，而且不知道这样的规定还有多少个

### 7.2.2 Simple Snooping System Model: Atomic Requests, Atomic Transactions
The **Atomic Requests** property states that a coherence request is *ordered in the same cycle* that it is issued.
This property eliminates the possibility of a block’s state changing—due to another core’s coherence request—between when a request
is issued and when it is ordered
> order到底是什么什么操作，为什么issue和order之间是做什么的
> 由于数据中间，


The **Atomic Transactions** property states that coherence transactions
are atomic in that a subsequent request for the same block may not appear on the bus until after the
first transaction completes (i.e., until after the response has appeared on the bus).
> 请求数据和数据到达含有时间延迟，当数据没有到位的时候，不会存在请求数据

### 7.2.2.1 Detailed Protocol Specification
This protocol has very
few transient states because the atomicity constraints of the simple system model greatly limit the
number of possible message interleavings
> atomicity constrais transient states，not apperant

The system model’s atomicity properties simplify cache miss handling in two ways.
1. First, the Atomic Requests property ensures that when a cache controller seeks to upgrade permissions to a
block—to go from I to S, I to M, or S to M—it can issue a request without worrying that another
core’s request might be ordered ahead of its own. Thus, the cache controller can transition immediately to state ISD, IMD, or SMD, as appropriate, to wait for a data response.
2. Similarly, the Atomic
Transactions property ensures that no subsequent requests for a block will occur until after the current transaction completes, eliminating the need to handle requests from other cores while in one
of these transient states

*A cache that has a block in state S can ignore GetS requests because the memory
controller is required to respond, but must invalidate the block on GetM requests to enforce the
coherence invariant*. A cache that has a block in state M must respond to both GetS and GetM
requests, sending a data response and transitioning to state S or state I, respectively
> 重要的描述

The LLC/memory has two stable states, M and IorS, and one transient state IorS<sup>D</sup> . In state
IorS, the memory controller is the owner and responds to both GetS and GetM requests because
this state indicates that no cache has the block in state M. In state M, the memory controller does
not respond with data because the cache in state M is the owner and has the most recent copy of
the data. However, a GetS in state M means that the cache controller will transition to state S, so
the memory controller must also get the data, update memory, and begin responding to all future
requests. It does this by transitioning immediately to the transient state IorS<sup>D</sup> and waits until it
receives the data from the cache that owns it.
> 重要的描述

In this protocol, the
S-to-I downgrade is performed “silently” in that the block is evicted from the cache without any
communication with the other coherence controllers

At the LLC, the block enters state IorS<sup>D</sup> when the PutM request arrives, then
transitions to state IorS when the Data message arrives.

The **Atomic Requests** property simplifies the
cache controller, by preventing an intervening request that might downgrade the state (e.g., another
core’s GetM request) before the PutM gets ordered on the bus. Similarly, the **Atomic Transactions**
property simplifies the memory controller by preventing other requests for the block until the PutM
transaction completes and the memory controller is ready to respond to them.
> 再一次说明原子操作属性 让　操作　变得简单
> 再一次无法理解这两个原子操作的含义是什么


#### 7.2.2.2 Running Example
> skip the example, but I know this Table7.7 is not easy.

### 7.2.3 Baseline Snooping System Model: Non-Atomic Requests, Atomic Transactions
Non-atomic requests arise
from a number of implementation optimizations, but most commonly due to inserting a message
queue (or even a single buffer) between the cache controller and the bus.
> 插入缓冲区，导致issue和order的时间不一致，但是如果不插入缓冲区，会有什么影响


Relaxing the Atomic Requests property introduces numerous situations in which a cache controller observes a request
from another controller on the bus in between issuing its coherence request and observing its own coherence request on the bus
> 似乎理解了issue 和 order的含义，以及中间含有时间差，但是那又怎样，看**Table7.8** 和 **Table7.9**

Until the requesting cache controller’s own GetS is observed on the bus and serialized, the block’s state is *effectively*
> logically可以理解，但是effectively想要表达什么，是indeed in fact的含义吗?


The cache controller changes the block’s state to IS<sup>D</sup> and waits for the data response from the previous owner. Because
of the Atomic Transactions property, the data message is the next coherence message (to the same block).

The transition from S to M illustrates the potential for state changes to occur during the
window of vulnerability.
> 这一个例子中间解释了，在一个Core想要GetM和这条消息发送出去，中间可能含有很长的时间，如果中间含有其他消息打断，那么就可能不能正常的执行
> 如果保证是原子性，那么这一个请求从issue到order之前就没有被打断的可能

Sidebar: upgrade Transactions in Systems Without Atomic Requests
> skip
> 显然，这一个Sidebar有点用
> Upgrade 操作?

The window of vulnerability also affects the M-to-I coherence downgrade, in a much more
significant way.
> skip the following explanation

### 7.2.4 Running Example
> skip this example

### 7.2.5 Protocol Simplifications
1. The most significant simplification is the use of atomic transactions on the bus
2. Another notable simplification that sacrifices performance involves the event of a store request to a cache block in state S
> 回顾sidebar中间关于upgrade的东西


## 7.3 Adding the Owned State

## 7.4 Adding the  State

## 7.5 Non-Atomic Bus
### 7.5.1 Motivation
The *SGI Challenge* enforced atomic transactions on a pipelined bus using a fast table lookup to check
whether or not another transaction was already pending for the same block
> SGI Challenge 是什么东西

### 7.5.2 In-Order vs. Out-of-order Responses
| Category          | Def                           | matching response with requests                    |
|-------------------|-------------------------------|----------------------------------------------------|
| atomatic          | wait for responese            | most recent request                                |
| pipelined         | don't wait  for response      | keep track of the number of outstanding requests   |
| split-transaction | response with different order | carry the identity of the request or the requestor |

### 7.5.3 Non-Atomic System Model
> STOP 所以trasaction atomic到底是什么

Each coherence controller has connections to and from both
buses, with the exception that the memory controller does not have a connection to make requests.

We draw the FIFO queues for buffering incoming and outgoing messages because it is important
to consider them in the coherence protocol.
### 7.5.4 An MSI Protocol with a Split-Transaction Bus
All of these newly possible transitions are for blocks in transient states
in which the cache is **awaiting a data response**; while waiting for the data, the cache first observes
another coherence request for the block
> 之前认为，发出请求之后，就可以立刻得到数据，但是实际上是，需要话费很多周期之后才可以得到数据
> 介绍的这两种原子性让我觉得是不是还可能有其他的原子性的存在
> 总线，到底是什么?是不是就是一条线
> 有没有办法试验这里面说的东西

For the other newly possible transitions, at both the cache controller and the memory controller, we also choose to stall until data arrives to satisfy the in-flight request. This is the simplest
approach, but it raises three issues.
1. First, it sacrifices some performance
2. stalling raises the potential of deadlock.
3. the third issue raised by stalling coherence requests is that, perhaps surprisingly, it enables a requestor to observe a response to its request before processing its own request

> Oh my god, feels too bad
