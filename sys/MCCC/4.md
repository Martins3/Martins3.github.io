### 4.1 MOTIVATION FOR TSO/x86
> 此处的store buffer保存的是commited之后的store
> 同一个core之前操作有依赖关系，应该是被处理了，不同core之前，操作时间不重要，只要不存在多个人同时持有锁就可以了

the write buffer thus hides the latency of servicing a store miss

For a single-core processor, a write buffer can be made architecturally invisible by ensuring
that a load to address A returns the value of the most recent store to A even if one or more stores to
A are in the write buffer. This is typically done by either bypassing the value of the most recent store
to A to the load from A, where “most recent” is determined by program order, or by stalling a load
of A if a store to A is in the write buffer.
> 无论如何，问题都是可以被处理的

When building a multicore processor, it seems natural to use multiple cores, each with its own
bypassing write buffer, and assume that the write buffers continue to be architecturally invisible
> SC需要保证的东西是什么，所有的处理器访问内存就像使用一个选择器，显然这种假是不够的，除非是一条指令作为一个单位。
> P38的例子就说明了，即使没有Cache也是会造成问题

Without write buffers, the hardware is SC, but with write buffers, it is not,
making write buffers architecturally visible in a multicore processor.

The option chosen by SPARC and later x86 was to abandon SC in favor of a memory consistency model that allows
straightforward use of a first-in–first-out (FIFO) write buffer at each core.
The new model, TSO, allows the outcome “(r1, r2) = (0, 0).” This model astonishes some people
but, it turns out, behaves like SC for most programming idioms and is well defined in all cases.
> 放弃write buffer是不可能的
> 为什么需要使用FIFO
> 令人窒息的结果还是有的，但是如何保证正常的运行（⊙ｏ⊙）？

### 4.2 BASIC IDEA OF TSO/x86
As execution proceeds, SC requires that each core preserves the program order of its loads and stores
for all four combinations of consecutive operations:
1. Load → Load
1. Load → Store
1. Store → Store
1. Store → Load /* Included for SC but omitted for TSO */
> 顺序的保证是通过是什么实现的?

More generally, TSO behaves the same as SC for common programming idioms that follow
1. C1 loads and stores to memory locations D1, . . ., Dn (often data),
2. C1 stores to F (often a synchronization flag) to indicate that the above work is complete,
3. C2 loads from F to observe the above work is complete (sometimes spinning first and often using a read–modify–write instruction), and
4. C2 loads and stores to some or all of the memory locations D1, . . ., Dn.

Omitting the fourth constraint allows each core to use a
write buffer. Note that the third constraint means that the write buffer must be FIFO (and not, for
example, coalescing) to preserve store–store order.

Programmers (or compilers) can prevent the execution in Figure 4.2(d) by inserting a
FENCE instruction between S1 and L1 on core C1 and between S2 and L2 on core C2. Executing
a FENCE on core Ci ensures that Ci’s memory operations before the FENCE (in program order)
get placed in memory order before Ci’s memory operations after the FENCE.

TSO does allow some non-intuitive execution results
> Table再次说明了重要的问题，对于single-Thread, 顺序总是保证的

### 4.3 A LITTLE TSO FORMALISM AND AN x86 CONJECTuRE
In this section we define TSO more precisely with a definition that makes only three changes to the
SC definition of Section 3.5.
A TSO execution requires:
1. All cores insert their loads and stores into the memory order \<m respecting their program
order, regardless of whether they are to the same or different addresses (i.e., a==b or a!=b). There
are four cases
2. Every load gets its value from the last store before it to the same address
3. Part (1) must be augmented to define FENCEs: /* Change 4: FENCEs Order Everything */

> Part(1), emmm, where is it ?

### 4.4 IMPLEMENTINg TSO/x86
The implementation story for TSO/x86 is similar to SC with the addition of per-core FIFO write
buffers. Figure 4.4(a) updates the switch of Figure 3.3 to accommodate TSO and operates as
follows:
1. Loads and stores leave each core in that core’s program order \<p.
2. A load either bypasses a value from the write buffer or awaits the switch as before.
2. A store enters the tail of the FIFO write buffer or stalls the core if the buffer is full.
2. When the switch selects core Ci, it performs either the next load or the store at the head
of the write buffer

In Section 3.7, we showed that, for SC, the switch can be replaced by a cache coherent
memory system and then argued that cores could be speculative and/or multithreaded and that nonbinding prefetches could be initiated by cores, caches, or software.
> switch通过cache coherent memory system实现
> nonbinding prefetches到底是一个什么鬼东西

Moreover, most current TSO implementations seem to use only the above
approach: take an SC implementation and insert write buffers

 For example, microarchitectures can physically
combine the *store queue* (uncommitted stores) and write buffer (committed stores), and/or physically separate load and store queues.
> 原来两个的名字不一样啊

Finally, multithreading introduces a subtle write buffer issue for TSO
> 此处的multithreading表示什么含义，难道不是process的切换吗? 为什么不同的不可以传递数值?

### 4.5 ATOMIC INSTRuCTIONS AND FENCES WITH TSO

##### 4.5.1 Atomic Instructions
The key difference is that TSO allows loads to pass (i.e., be ordered before)
earlier stores that have been written to a write buffer. The impact on RMWs is that the “write” (i.e.,
store) may be written to the write buffer.

Because the load part of the RMW cannot be performed until earlier stores have been ordered (i.e., exited the write buffer),
the atomic RMW effectively drains the write buffer before it can perform the load part of the RMW
> 从流水线的角度来说，指令总是按照程序顺序提交的，但是write buffer的存在，导致从内存的角度上来说，指令提交的顺序不是一致的
> 所以一旦清空了write buffer中间的指令，那么就可以保证没有load store完成
> 原子指令: 什么样的指令才可以称为原子指令?
> 1. 两条操作之前不可以插入其他操作，是自己不可以插入，还是别人也不可以插入操作
> 2. 应该是其他的core也不可以插入操作，两个core同时读，同时满足，同时获取锁
> 3. 但是感觉难以完成这一个操作啊

More optimized implementations of RMWs are possible. For example, the write buffer does
not need to be drained as long as (a) every entry already in the write buffer has read–write permission in the cache and maintains the read–write permission in the cache until the RMW commits
and (b) the core performs MIPS R10000-style checking of load speculation (Section 3.8).
> 为什么持有了权限就就可以不清空wirte buffer


### 4.5.2 FENCEs
. Without these FENCEs, the two loads (L1 and L2) can bypass the two stores (S1 and S2),
leading to an execution in which r1 and r2 both get set to zero.
A simple implementation—such as draining
> Oh my god, bypass

the write buffer when a FENCE is executed and not permitting subsequent loads to execute until
an earlier FENCE has committed—may provide acceptable performance
### 4.6 FuRTHER READINg REgARDINg TSO
> skip this short paragraph

### 4.7 COMPARINg SC AND TSO
What Is a good Memory Consistency Model?
A good memory consistency model should possess Sarita Adve’s 3Ps [1] plus our fourth P:
1. Programmability: A good model should make it (relatively) easy to write multithreaded
programs. The model should be intuitive to most users, even those who have not read the
details. It should be precise, so that experts can push the envelope of what is allowed.
2. Performance: A good model should facilitate high-performance implementations at reasonable power, cost, etc. It should give implementors broad latitude in options.
3. Portability: A good model would be adopted widely or at least provide backward compatibility or the ability to translate among models.
4. Precision: A good model should be precisely defined, usually with mathematics. Natural
languages are too ambiguous to enable experts to push the envelope of what is allowed
